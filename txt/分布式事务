分布式事务



分布式：一个业务拆分成多个子业务，每个子业务分别部署在不同的服务器上。每个子系统被称为服务。这些子系统独立运行，之间通过rpc/http进行通信
微服务：微服务的设计是为了不因为某个模块的升级和Bug影响现有的业务系统，即将模块功能进行拆分。


分布式：分散压力
微服务：分散能力

分布式一定是微服务，微服务不一定是分布式。
分布式的定义：把一个服务拆分成多个子服务，分别放在不同的服务器上。微服务可以放在同一个服务器上，也可以放在不同的服务器上。


集群：同一个业务，部署在多个服务器上






分布式 主要为了解决 高并发问题，
集群主要为了解决 高可用问题。





一、
1、分布式ID（雪花算法）
2、分布式锁（redis/zookper/etcd）
3、分布式事务
4、分布式缓存(参考 https://github.com/zlbonly/geecache )




四、分布式缓存实现 自实现（参照groupcache）

1、FIFO 先进先出
	队列：队列是一种特殊的线性表，只允许在队头进行删除元素，在队尾进行插入。

2、缓存淘汰（失效）算法。FIFO,LFU 和LRU
	概述： 
	GeeCache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？
  
  2.1 
  	FIFO (First In  First Out) 先进先出
  	先进先出，也就是淘汰缓存中最老（即最早）添加的的记录。FIFO认为，最早添加的记录，其不再被使用的可能性难过比刚添加的可能性大。这种算法的实现比较简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景中，部分记录虽然最早被添加，但是也是最常被访问的，而不得不因为待的时间太长而被淘汰。这类数据会被频繁的添加进缓存中，又被淘汰出去。导致缓存命中吕降低。
  
  2.2 
  	LFU (Least Frequently Used) 最少使用
  	最少使用。也就淘汰缓存中访问频率最低的记录。LFU认为，如果数据过去被访问多次，那么将来被访问的频率也更高。LFU的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU算法的命中率是比较高的，但是缺点也比较明显，维护每个记录的访问次数，对内存的消耗是很高的；另外，如果数据的访问模式发生变化，LFU需要较长的时间去适应，也就是说LFU算法受历史数据的影响比较大。例如：某个数据历史上访问次数奇高，但是在某个时间点之后就不再被访问了，但是因为历史访问次数过高，而迟迟不能被淘汰。

  2.3 
  	LRU (Least Recently Used)
  	最近最少使用。相对于仅考虑时间因素的FIFO 和 仅考虑访问频率的LFU，LRU算法可以认为是相对平衡的一种淘汰算法。LRUR认为
  	如果数据最近被访问过，那么将来被访问的频率也会更高。LRU算法的实现非常简单。维护一个队列，如果某条记录被访问了，则移动到队尾，
 	那么队首则是最近最少访问的数据了，淘汰该记录就行。

  2.4 LRU 算法实现

  数据结构：字典 key-value，例如： map[key]value 和 双向链表
  说明： 1、字典map，存储键和值的映射关系。这样可以根据某个键（key）查找对应的值value的复杂度是O(1)，在字典中插入一条记录的复杂		  度也是O(1)
        2、使用双向链表实现队列。将所有的值放到双向链表中，这样，访问到某个值时，将其移动到队尾的时间复杂度是O(1)，在队尾新增一条记录以及删除一条记录的复杂度均为O（1）。


    具体实现参照：(https://github.com/zlbonly/simple-go-algorithm/blob/master/lru/lru.go 和 https://geektutu.com/post/geecache.html）
   	2.4.1 数据结构定义
   		
	2.4.2 查找
	如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）

	2.4.3  删除
	c.ll.Back() 取到队首节点，从链表中删除。
delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
更新当前所用的内存 c.nbytes。
	2.4.4 新增
	如果键存在，则更新对应节点的值，并将该节点移到队尾。
不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。

4、一致性哈希实现

    	一致性哈希算法是啥？为什么要使用一致性哈希算法？这和分布式有什么关系？

    	举例子：

    	1.1 我该访问谁？

   		对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

		假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

		那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

		1.2 节点数量变化了怎么办？

		简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 hash(key) % 10 变成了 hash(key) % 9，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 缓存雪崩

		那如何解决这个问题呢？一致性哈希算法可以。


		算法原理：  参考链接：（https://geektutu.com/post/geecache-day4.html）
		一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

		1、计算节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值，放置在环上。
		2、计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

		也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

		2.2 数据倾斜问题

		如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均。

		为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

		假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

		第一步，计算虚拟节点的 Hash 值，放置在环上。
		第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

		虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

		算法实现（go） 参考：https://github.com/zlbonly/simple-go-algorithm/blob/master/consistent/hash/consistenthash.go
