分布式事务


分布式：一个业务拆分成多个子业务，每个子业务分别部署在不同的服务器上。每个子系统被称为服务。这些子系统独立运行，之间通过rpc/http进行通信
微服务：微服务的设计是为了不因为某个模块的升级和Bug影响现有的业务系统，即将模块功能进行拆分。


分布式：分散压力
微服务：分散能力

分布式一定是微服务，微服务不一定是分布式。
分布式的定义：把一个服务拆分成多个子服务，分别放在不同的服务器上。微服务可以放在同一个服务器上，也可以放在不同的服务器上。


集群：同一个业务，部署在多个服务器上


分布式 主要为了解决 高并发问题，
集群主要为了解决 高可用问题。





一、
1、分布式ID（雪花算法） 参考： https://github.com/zlbonly/simple-go-algorithm/tree/master/snowflake2/uniqueid
2、分布式锁（redis/zookper/etcd）
3、分布式事务
4、分布式缓存(参考 https://github.com/zlbonly/geecache )


二、 分布式锁

	2.1  基于redis的 setnx实现分布式锁
		基于Redis 实现分布式锁 (setnx) setnx也可以存入key,如果存入key成功则返回1，如果存入的key已经存在了，返回0.


	缺点： 自认为 只适用单实例的redis分布式锁，如果存在redis集群（主从集群，cluster集群）等，可能会存在问题（例如，master 突然挂掉，缓存key 还没有来得及同步到slave）,redis官方适用redlock算法来解决。参考java 封装的redission

	redlock算法 大致：

	公司量级不够，目前redis仅一台服务器，单实例配置，只有需要同步数据的才 进行主从配置。（一主多从）

	2.2、基于Zookerper 实现分布式锁


	2.3、基于etcd

	    1、etcd 实现分布式锁 原理
        	 		1、先检查/lock路径下是否有值，如果有值，说明锁已经被占用了，
        	 		2、如果没有值，写入自己的值。写入成功返回，说明加锁成功，如果写入时节点被其他节点写入过了，那么会导致加锁失败
        	 		，跳到第三步
        	 		3、监视/lock 下的时间，先入zuse
        	 		4、当/lock路径下发生事件时，当前进程被唤醒，检查发生的时间是否是删除事件（说明锁持有着主动解锁） 或者过期事件
        	 		(说明锁过期失效) 如果是的话，那么回到1，走抢锁流程。

        	 		etcd v3 api 官方已经提供了可直接使用的锁 API
        	 	 参考： https://github.com/zieckey/etcdsync/blob/master/mutex.go 实现：

        	 	 demo :使用上，跟 Golang 官方 sync 包的 Mutex 接口非常类似，先New()，然后调用Lock()，使用完后调用Unlock()，

        		 	 func main() {
        		//etcdsync.SetDebug(true)
        		log.SetFlags(log.Ldate|log.Ltime|log.Lshortfile)
        		m := etcdsync.New("/etcdsync", "123", []string{"http://127.0.0.1:2379"})
        		if m == nil {
        		log.Printf("etcdsync.NewMutex failed")
        		}
        		err := m.Lock()
        		if err != nil {
        		log.Printf("etcdsync.Lock failed")
        		} else {
        		log.Printf("etcdsync.Lock OK")
        		}

        		log.Printf("Get the lock. Do something here.")

        		err = m.Unlock()
        		if err != nil {
        		log.Printf("etcdsync.Unlock failed")
        		} else {
        			log.Printf("etcdsync.Unlock OK")
        		}
        	}

	4.4、数据库乐观锁 (给数据库表添加相应的version版本控制)
	     ectd 内部也是 MVCC分布式锁




	2.2 redis实现分布式锁
	 2.2.1 分布式锁实现需要满足的条件
	  	 1. 互斥性。在任意时刻，只有一个客户端能够持有锁。
	  	 2。不会发生死锁。即时有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证其他客户端能加锁。
	  	 3.具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁
	  	 4。加锁和解锁必须是同一个客户端。客户端自己不能吧别人加的锁给解锁了。


	 正确实现1：使用多参数的set() 使用redis.set(key,value,NX,px,expire_time);
	 错误实现2: 使用redis.setnx()命令加锁，人后使用expire()方法设置过期时间。
		eg: if(redis.setnx(key)){
					reids.expire(key,expireTime);
				}

			问题：setnx（）和expire（）是两条Redis命令，不具备原子性，如果程序在执行setnx（）之后突然崩溃，会导致
			锁没有设置过期时间。将会发生死锁。网上有人这么实现，因为低版本的redis不支持多参数的set()

	错误实现3:
			 实现思路：使用jedis.setnx()命令实现加锁，其中key是锁，value是锁的过期时间。执行过程：1. 通过setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。2. 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功

			 存在问题：
			 	1. 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 2. 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。3. 锁不具备拥有者标识，即任何客户端都可以解锁。
	 正确实现4:

	 		   String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";

        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        使用简单的lua脚本，然后将Lua代码传递到reids.eval()方法中，通过参数赋值相应的lockKey。eval()方法是将Lua代码交给Redis服务端执行，redis服务端可以确保eval()方法的原子性。


        以上策略都是单机Redis实现分布式锁，如果redis集群Master-slave会存在安全性问题：
        	例如：
        		 1、客户端1 从Master获取了锁
        		 2、Master宕机了，存储锁的key还没有来得及同步到Slave上
        		 3、Slave升级为Master
        		 4、客户端2从新的Master获取到了对应同一个资源的锁
        		 于是，客户端1和客户端2同时持有了同一个资源的锁，锁的安全型被打破


        针对redis集群多服务实例场景，可以使用Redlock算法
        可以参考Redisson实现分布式锁，Redis官方提供的分布式锁组件，内部使用了ReadLock算法。
           go的redsync包实现了redlock算法。（https://github.com/go-redsync/redsync）

        	ReadLock红锁算法：
        	1、获得当前时间（ms）

			2、首先设置一个锁有效时间valid_time,以及获取锁的 花费的时间，（获取锁花费时间 要比valid_time小很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。）
			client尝试按照顺序使用相同的key,value获取所有redis服务的锁，
			比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁

			3、计算获取锁总共占用的时间，再加上时钟偏移，如果这个总时间远小于valid_time，并且成功设置锁的实例数>= N/2 + 1，那么加锁成功

			4、如果加锁成功了，那么这个锁的有效时间就是valid_time - 获取锁占用的时间 - 时钟偏移

			5、如果加锁失败，解锁所有实例（每个redis实例都运行del key）

            补充：时钟漂移；指两个电脑间时间流速基本相同的情况下，两个电脑（或两个进程间）时间的差值；如果电脑距离过远会造成时钟漂移值 过大

三、分布式事务

	2.3 分布式事务 基础理论

		2.3.1 事务
		事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。事务应该具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性。

		2.3.2 分布式事务

			分布式事务是指事务的参与者，支持事务的服务器，资源服务器及事务管理器分别位于不同的分布式系统的不同节点上。
			例如：下单接口会扣减库存，生成订单，支付状态等，不仅仅取决于本地的db操作，而且依赖第三方系统的结果。
			这时候分布式事务就保证这些操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。

    CAP理论

 1、CAP理论 Consistency (一致性)，Availability （可用性），Partition Tolerance（分区容错性）

    一致性：所有节点访问同一份最新的数据副本
    可用性：非故障节点在合理的时间内返回合理的响应（不是错误或者超时的响应）
    分区容错性：分布式系统出现网络分区/网络故障的时候，仍然能够对外提供服务。

    网络分区：
        分布式系统中，多个节点之间的网络本来是联通的，但是因为某些故障（比如部分节点网络出现了问题）某些节点之间不连通了，
        整个网络就分成了几块区域，这就叫网络分区。

        当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。
        简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。


  2、分布式系统 保证 CP 和 AP ，无法保证 CA
    1、为啥无同时保证 CA 呢？
      举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，
      这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。


      ZooKeeper 保证的是 CP。 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。


 3、BASE 理论思想

    BASE 理论是对CAP中一致性C 和 可用性A权衡的结果，是基于CAP定理逐步演化而来的，具体指，Basically Available（基本可用），Soft-state(软状态)，
    和 Eventually Consistent(最终一致性)。 核心思想是，即使无法做到强一致性，但每个应用可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。

     注意：BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。


    1. 基本可用
        基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。
        什么叫允许损失部分可用性呢？

        响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
        系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。

     2. 软状态
            软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。
     3. 最终一致性
        最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

        分布式一致性的 3 种级别：
           强一致性 ：系统写入了什么，读出来的就是什么。
           弱一致性 ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。
           最终一致性 ：弱一致性的升级版。，系统会保证在一定时间内达到数据一致的状态，

           ！！！ 业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。


		2.3.2 分布式事务使用场景：
		1、转账：
			转账是最经典那的分布式事务场景，假设用户 A 使用银行 app 发起一笔跨行转账给用户 B，银行系统首先扣掉用户 A 的钱，然后增加用户 B 账户中的余额。此时就会出现 2 种异常情况：1. 用户 A 的账户扣款成功，用户 B 账户余额增加失败 2. 用户 A 账户扣款失败，用户 B 账户余额增加成功。对于银行系统来说，以上 2 种情况都是不允许发生，此时就需要分布式事务来保证转账操作的成功。
		2、下单扣库存
			在电商系统中，下单是用户最常见操作。在下单接口中必定会涉及生成订单 id, 扣减库存等操作，对于微服务架构系统，订单 id 与库存服务一般都是独立的服务，此时就需要分布式事务来保证整个下单接口的成功。


		2.3.3 分布式事务解决方案

			1、两段式提交2PC(Two-Phase Commit)
                定义：
                    两段式提交协议目标在于为分布式系统保证数据的一致性。它将分布式事物的提交过程分为两个阶段，1、投票，2、事务提交
                    为了让整个数据库集群能够正常的运行，该协议指定了一个 【协调者】单点，用于协调整个数据库集群各个节点的运行。
                    为了简化描述将数据库集群中的各个节点成为 【参与者】。
                大致流程：
                    1、第一阶段 投票
                        该阶段的主要目的是在于打探数据库集群中的各个参与者能否够正常执行事务，具体步骤如下：
                        1、协调者向所有的参与者发送事务执行请求，并等待参与者反馈事务执行的结果。
                        2、事务参与者收到请求之后，执行事务但是不提交，并记录事务的日志。
                        3、参与者将自己事务执行情况反馈给协调者，同时阻塞等待协调者的后续指令。

                     2、第二阶段 事务提交
                        在经过第一阶段协调者的询盘之后，各个参与者会回复自己事务执行的情况，这时候存在3中可能性。
                        1、所有的参与者都能够正常执行事务
                        2、一个或者多个参与者回复事务执行失败
                        3、协调者等待超时

                        针对 第2.1种情况，协调者将向所有的参与者发出提交事务的通知，具体步骤如：
                            1、协调者向各个参与者发送commit通知，请求提交事务
                            2、参与者收到事务提交通知之后执行commit操作，然后释放占用的资源
                            3、参与者向协调者返回事务commit结果信息
                            如流程图：https://github.com/zlbonly/simple-go-algorithm/blob/master/pics/2PC-commit1.png

                        针对 第2.2 和 2.3 协调者均认为参与者无法成功执行事务，为了整个集群数据的一致性，所以要向各个参与者发送事务回滚通知，
                        具体步骤如下：
                            1、协调者向各个参与者发送事务rollback通知，请求回滚事务。
                            2、参与者收到事务回滚通知之后执行rollback操作，然后释放占有的资源。
                            3、参与者向协调者返回事务rollback结果信息。
                            流程信息：
                            如流程图：https://github.com/zlbonly/simple-go-algorithm/blob/master/pics/2PC-commit2.png

                两段式提交协议解决的是分布式数据库数据强一致性问题，原理简单，易于实现，但是存在一下缺点：
                1、单点问题
                    协调者在整个两阶段提交过程中扮演着举足轻重的作用，一旦协调者所在服务器宕机，就会影响整个数据库集群的正常执行。
                    比如在第二阶段中，如果协调者因为故障不能正常发送事务提交或者回滚通知，那么参与者将一直处于阻塞状态，真个数据库集群将无法提供服务

                2、同步阻塞
                    两段式提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他从操作，效率低下。

                3、数据不一致性
                    两段提交协议虽然是分布式数据强一致性所设计，但仍然存在者数据不一致性的可能性。比如在第二阶段中，假设协调者发出了事务commit通知，但是因为网络问题该通知仅被
                    一部分参与者所收到并执行了commit操作，其余的参与者因为没有收到一直处于阻塞状态，这时候就产生了数据的不一致性。


                 上述问题可以通过 超时机制 很大程度解决。

            2、三段式提交3PC（Three-Phase Commit）

                定义：
                    针对两段式协议存在的问题，三段式协议通过引入一个 【预询盘】阶段，以及超时策略来减少整个集群的阻塞时间，提升系统性能
                    三段式提交的三个阶段分别为：
                        1、【预询盘】（can_commit）
                        2、【预提交】（pre_commit）
                        3、【事务提交】（do_commit）

                     1、第一阶段 预询盘
                        该阶段协调者会去询问各个参与者是否能够正常执行事务，参与者根据自身情况回复一个预估值，相对于真正执行的事务，
                        这个过程是轻量的，具体步骤如下：
                            1、协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待回复。
                            2、各个参与者依据自身情况回复一个预估值，如果预估自己能够正常执行事务就返回确定信息，并进入预备状态，否者返回否定信息。

                     2、第二阶段 预提交
                            本阶段协调者会根据第一阶段的询盘结果采取相应的操作，询盘结果主要有3种：
                            1、所有的参与者都返回确定的信息
                            2、一个或多个参与者返回否定信息
                            3、协调者等待超时

                         针对2.1 情况，协调者会向所有的参与者发送事务执行请求，具体步骤如下：
                            1、协调者向所有的事务参与者发送事务执行通知，
                            2、参与者收到通知后执行事务但是不提交
                            3、参与者将事务执行情况返回给客户端。

                         在上述步骤中，如果参与者等待超时，则会中断事务。针对第2和第3情况，协调者认为事务无法正常执行，于是向各个参与者发送abort通知
                         ，请求推出预备状态。

                     3、第三阶段： 事务提交
                        如果第二阶段事务未中断，那么本阶段协调者将会根据事务执行的返回结果来决定提交或者回滚事务，分为3中情况。
                           1、所有的参与者都能正常执行事务
                           2、一个或者多个参与者执行事务失败
                           3、协调者等待超时

                        针对3.1种情况，协调者向各个参与者发起事务提交请求，具体步骤如下：
                            1、泄题者向所有的参与者发送事务commit通知；
                            2、所有参与者在收到通知之后执行commit操作，并且释放占有的资源，
                            3、参与者向协调者反馈事务提交的结果。

                        针对3.2 和 3.3种情况 协调者认为事务无法成功执行，于是向各个参与者发送事务回滚请求，具体步骤如下：
                            1、协调者向所有参与者发送事务rollback通知
                            2、所有参与者在收到执行rollback操作，并且释放占有资源
                            3、参与者向协调者反馈事务回滚结果。

                         在本阶段中 如果因为协调者或者 网络原因，导致参与者迟迟不能收到来自协调者的commit或rollback请求
                         那么参与者将不会如两阶段提交中那样陷入阻塞，而是在等待超时后继续commit，相对于两阶段提交虽然降低了同步阻塞，
                         但是仍然无法完全避免数据的不一致。

                         说明：两段提交协议中所存在的长时间阻塞状态发送的几率还是很低的，所以虽然三段式协议相对于两段式协议对于
                         数据一致性更保障，但是因为效率问题，两段式提交在实际系统中反而更加受宠。

                               参考连接：【链接】分布式事务：两阶段提交与三阶段提交
                             https://segmentfault.com/a/1190000012534071

			    3、TCC 补偿式提交 （try-confirm-cancel）

                       其核心思想是：针对每个业务，都要注册一个与其对应的确认和补偿（撤销）机制，
					    TCC(Try Confirm Cancel)
                   1、 Try 阶段主要是对业务系统做检测及资源预留
                   2、 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。
                   3、 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。
                    举个例子，假入 Bob 要向 Smith 转账，思路大概是：我们有一个本地方法，里面依次调用
                    1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。
                    2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
                    3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

                   缺点：每个业务都需要实现 try,confirm，cancel ，实现难度太大。

			4、本地消息表

			5、可靠消息一致性 （大致是使用MQ）

				1、A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作
				2、如果消息发送成功，则执行本地事务
				3、如果本地事务执行成功，则想 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息
				4、B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 		系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求
				5、mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息


			该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与余容错方案基本一致。目前市面上实现该方案的只有阿里的 RocketMq


			5、尽最大努力通知
				尽最大努力通知是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。

				这个方案的大致意思就是：

				系统 A 本地事务执行完之后，发送个消息到 MQ；
				这里会有个专门消费 MQ 的服务，这个服务会消费 MQ 并调用系统 B 的接口；
				要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃


分布式事务参考：小米团队的总结 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/
总结： 分布式锁是解决并发时资源争抢的问题，分布式事务和本地事务是解决流程化提交问题。





四、分布式缓存实现

4.1 常用的分布式缓存，groupcache /memcache/redis

4.2 自实现分布式缓存（参照groupcache）
	1、LUR 缓存淘汰策略
	2、单机并发缓存
	3、HTTP服务端
	4、一致性哈希
	5、分布式节点
	6、防止缓存击穿
	7、支持protobuf通信


1、Lru缓存策略
1.1、FIFO 先进先出
	队列：队列是一种特殊的线性表，只允许在队头进行删除元素，在队尾进行插入。

1.2、缓存淘汰（失效）算法。FIFO,LFU 和LRU
	概述：
	GeeCache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？

  1.2.1
  	FIFO (First In  First Out) 先进先出
  	先进先出，也就是淘汰缓存中最老（即最早）添加的的记录。FIFO认为，最早添加的记录，其不再被使用的可能性难过比刚添加的可能性大。这种算法的实现比较简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景中，部分记录虽然最早被添加，但是也是最常被访问的，而不得不因为待的时间太长而被淘汰。这类数据会被频繁的添加进缓存中，又被淘汰出去。导致缓存命中吕降低。

  1.2.2
  	LFU (Least Frequently Used) 最少使用
  	最少使用。也就淘汰缓存中访问频率最低的记录。LFU认为，如果数据过去被访问多次，那么将来被访问的频率也更高。LFU的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU算法的命中率是比较高的，但是缺点也比较明显，维护每个记录的访问次数，对内存的消耗是很高的；另外，如果数据的访问模式发生变化，LFU需要较长的时间去适应，也就是说LFU算法受历史数据的影响比较大。例如：某个数据历史上访问次数奇高，但是在某个时间点之后就不再被访问了，但是因为历史访问次数过高，而迟迟不能被淘汰。

  1.2.3
  	LRU (Least Recently Used)
  	最近最少使用。相对于仅考虑时间因素的FIFO 和 仅考虑访问频率的LFU，LRU算法可以认为是相对平衡的一种淘汰算法。LRUR认为
  	如果数据最近被访问过，那么将来被访问的频率也会更高。LRU算法的实现非常简单。维护一个队列，如果某条记录被访问了，则移动到队尾，
 	那么队首则是最近最少访问的数据了，淘汰该记录就行。

  1.2.4 LRU 算法实现

  数据结构：字典 key-value，例如： map[key]value 和 双向链表
  说明： 1、字典map，存储键和值的映射关系。这样可以根据某个键（key）查找对应的值value的复杂度是O(1)，在字典中插入一条记录的复杂		  度也是O(1)
        2、使用双向链表实现队列。将所有的值放到双向链表中，这样，访问到某个值时，将其移动到队尾的时间复杂度是O(1)，在队尾新增一条记录以及删除一条记录的复杂度均为O（1）。


    具体实现参照：(https://github.com/zlbonly/simple-go-algorithm/blob/master/lru/lru.go 和 https://geektutu.com/post/geecache.html）
   	1.2.4.1 数据结构定义

	1.2.4.2 查找
		如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
		c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）

	1.2.4.3  删除
		c.ll.Back() 取到队首节点，从链表中删除。
		delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
		更新当前所用的内存 c.nbytes。
	1.2.4.4 新增
		如果键存在，则更新对应节点的值，并将该节点移到队尾。
		不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
		更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。


	目前常用的分布式缓存有memecache 和 redis


   4、一致性哈希实现

    	一致性哈希算法是啥？为什么要使用一致性哈希算法？这和分布式有什么关系？

    	举例子：

    	1.1 我该访问谁？

   		对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

		假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

		那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

		1.2 节点数量变化了怎么办？

		简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 hash(key) % 10 变成了 hash(key) % 9，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 缓存雪崩

		那如何解决这个问题呢？一致性哈希算法可以。


		算法原理：  参考链接：（https://geektutu.com/post/geecache-day4.html）
		一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

		1、计算节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值，放置在环上。
		2、计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

		也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

		2.2 数据倾斜问题

		如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均。

		为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

		假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

		第一步，计算虚拟节点的 Hash 值，放置在环上。
		第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

		虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

		算法实现（go） 参考：https://github.com/zlbonly/simple-go-algorithm/blob/master/consistent/hash/consistenthash.go



二、MQ 总结
	kafak RocketMQ RabbitMQ  Nsq(go语言实现的消息队列)，延迟队列


1、kafka

	1.1基本术语
	Message（消息）：传递数据的对象，主要由四部分组成，offset(偏移量)，key value，timestamp插入时间， 其中offset和timestamp 在kafka集群中产生，， value/key 在produce发送数据时产生

	Broker(代理者)：kafka集群中的机器/服务被称为broker是一个物理概念

	Topic(主题)：维护kafka的消息类型被称为Topic，是一个逻辑概念

	Partition(分区)： 具体维护Kafka扇消息数据的最小单位。一个Topic可以包含多个分区，

	Producer（生产者）： 负责将数据发送到对应的Kafka对应的Topic的进程

	Consumer（消费者）：负责从Topic获取数据的进程

	Consumer Group（消费者组）： 每个consumer都属于一个特定的group组，一个group组可以包含多个consumer，但是一个组中只会有一个consumer消费数据。

    1、AR 分区中所有的副本统称为AR
    2 所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR
    3、与leader 副本同步滞后过的副本（不包括leader副本）组成OSR.

    ISR 是AR 集合的一个子集。AR= ISR+OSR
	具体流程： 消息会先发送到leader副本，然后follower副本才能从Leader副本中拉取消息进行同步，
	同步期间内follower副本而言会有一定程度的滞后，与leader 副本同步滞后过的副本（不包括leader副本）组成OSR.

    4、消费者 和 消费者组
	消费者 ： 负责 订阅 kafka中的 主题（Topic），并且从订阅的主题上拉取消息。
	消费者组： 消费者组是kafka提供 横向扩展 消费能力的机制，组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。当然，每个分区只能由同一个消费组内的一个consumer来消费





	1.2 kafka 吞吐量，速度快的原因

		1.2.1 顺序读写

			kafka将消息记录 持久化到本地磁盘中，kafka的每一个Partition都是一个文件，在收到消息后Kafka会按顺序把数据追插入到文件末尾。

			说明（一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。）

		1.2.2 Page Cache (页缓存)
			为了优化读写操作性能，Kafka利用了操作系统本身的Page Cache，消息先被写入页缓存，然后由操作系统负责刷盘业务，利用操作系统本身的内存而不是Jvm内存空间，
			好处：
				1、避免了创建Object消耗： 如果使用Java堆，java对象内存消耗比较大，通常是所存储数据的两倍甚至更多
				2、避免GC问题：随着Jvm中数据不断增多，垃圾回收变得更加复杂且缓慢，使用系统缓存就不会存在Gc问题。
			通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。
				3、及时Kafka服务重启，页缓存还是会保持有效，进程内的缓存需要重建。


			Page Cache 介绍
				页缓存是操作系统实现的一个主要的磁盘缓存，以此来减少对磁盘I/O 的操作，具体来说就是 把磁盘中的数据缓存到内存中。具体流程
				读： 当一个进程准备读区磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页(page) 是否在页缓存中，
				如果存在（命中）则直接返回数据，从而避免了对物理磁盘的I/O操作； 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。
				写： 如果一个进程需要将数据写入磁盘，那么操作系统也会先检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入相应的页。被修改的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。

				linux 文件cache 分为两层，一个是 page cache ，另外一个是buffer cache ，每一个page cache 包含若干个buffer cache ,通过buffer cache中的指针 指向磁盘block。


		1.2.3 零拷贝

			linux操作系统 “零拷贝” 机制使用了sendfile()方法，允许操作系统将数据从Page Cache 直接 发送到网络，只需要最后一步的copy操作讲数据复制到NIC缓冲区，这样避免了重新复制。

			当Kafka客户端从服务器读取数据时，如果不使用零拷贝技术，那么大致需要经历这样的一个过程：

				1.操作系统将数据从磁盘上读入到内核空间的读缓冲区中。

				2.应用程序（也就是Kafka）从内核空间的读缓冲区将数据拷贝到用户空间的缓冲区中。

				3.应用程序将数据从用户空间的缓冲区再写回到内核空间的socket缓冲区中。

				4.操作系统将socket缓冲区中的数据拷贝到NIC缓冲区中，

				参考图： https://mmbiz.qpic.cn/mmbiz_jpg/hUzEz6BmcaovNVeibZibG1FrLHSNGBKXcLBxeYFPGTPskVaIk7DAZHn9H2Rf2elgBTqy6uSxsapvaxT3HnNVv1icQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1


				注意： 零拷贝并非指一次拷贝都没有，而是避免了在内核空间和用户空间之间的拷贝。



		1.2.4 分区分段 + 索引

			  Kafka的message是按topic分类存储的，topic中的数据又是按照一个一个的partition即分区存储到不同broker节点。每个partition对应了操作系统上的一个文件夹，partition实际上又是按照segment分段存储的。这也非常符合分布式系统分区分桶的设计思想。

			通过这种分区分段的设计，Kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，Kafka又默认为分段后的数据文件建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。


		1.2.5 批量读写

			Kafka数据读写也是批量的而不是单条的。

			除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多

		1.2.6 批量压缩
			在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。
			1、如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩
			2、Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩
			3、Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议

			kafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partion是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。

    1.3 kafka 的消费者位移（_consumer_offsets）
         1、位移主题（_consumer_offsets） 即 位移主题，Offsets Topic

    	1、0.11.0.0版本之前， Consumer 的位移管理是依托于 Apache ZooKeeper 的，它会自动或手动地将位移数据提交到 ZooKeeper 中保存。当 Consumer 重启后，它能自动从 ZooKeeper 中读取位移数据，从而在上次消费截止的地方继续消费。
    		缺点：由于Zookeeper并不适合大批量的频繁写入操作，消耗性能
    	2、	0.11.0.0版本之后，新版Kafka已推荐将consumer的位移信息保存在Kafka内部的topic中，即__consumer_offsets topic，并且默认提供了kafka_consumer_groups.sh脚本供用户查看consumer信息。

    		Consumer位移管理机制： Consumer 的位移数据作为一条条普通的 Kafka 消息，提交到 __consumer_offsets 中。可以这么说，__consumer_offsets 的主要作用是保存 Kafka 消费者的位移信息

    		位移主题的 Key 中应该保存 3 部分内容：<Group ID，主题名，分区号 >

    	3、kafka 删除过期位移。
    		kafka 使用compact策略 机制删除 位移主题 中的过期消息。
    		具体过程： kafka 通过Log Cleaner 后台线程 定期巡检Compact主题，删除过期的位移消息。


    1.4  kafka 为什么不支持读写分离
            在kafka中，生产者写入消息，消费者读取消息的操作都是与leader副本进行交互的，从而实现的是一种主写主读的生产消费模型。
           kafka不支持主写主读 的原因：
                1、数据一致性。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。
                2、延时问题。类似redis组件，数据从写入主节点到同步主节点的过程中需要经历 网络 -》 主节点内存-〉网络-》从节点内存
                这几个阶段。但是kafka 主从同步 会比redis等 更加耗时 ，他需要经历 网络 -》 主节点内存 -〉 主节点磁盘 -》网络-〉从节点内存 -》 从节点磁盘 这几个阶段。 对于延时敏感的应用而言，主写从读的功能并不适用。
                3、kafka的 主写主读的架构 可以达到很大程度删的负载均衡。基本可保证，每个broker都有消息从生产者流入，当消费者读取消息的时候也是从leader副本中读取。每个broker都有消息流出到消费者。

    1.5 kafka 事务和幂等性
        1、kafka幂等性
        		生产产者重复生产消息。生产者进行retry会产生重试时，会重复产生消息。有了幂等性之后，在进行retry重试时，只会生成一个消息。

        		幂层性特点：
        		1、只能保证 Producer 在单个会话内不丟不重，如果 Producer 出现意外挂掉再重启是无法保证的（幂等性情况下，是无法获取之前的状态信息，因此是无法做到跨会话级别的不丢不重）;
        		2、幂等性不能跨多个 Topic-Partition，只能保证单个 partition 内的幂等性，当涉及多个 Topic-Partition 时，这中间的状态并没有同步。
        		3、如果需要跨会话、跨多个 topic-partition 的情况，需要使用 Kafka 的事务性来实现。


        	具体原理： kafka 引入 Producer ID (PID) 和 Squence Number.
        		1、每个新的Producer在初始化的时候都会被分配一个唯一的PID，这个PID对用户不可见。
        		2、Sequence Number （对于每个PID ,该producer发送数据的每个消息都对应一个从0开始单调递增的Squence Number）
        		3、broker端在缓存中保存了这seq number，对于接收的每条消息，如果其序号比Broker缓存中序号大于1则接受它，否则将其丢弃。这样就可以实现了消息重复提交了


        	注意： 当幂等性开启的时候acks即为all。如果显性的将acks设置为0，-1，那么将会报错Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence.

        	2、kafka事务性
        		1、Kafka中的事务特性主要用于以下两种场景：

        		i) 生产者发送多条消息可以封装在一个事务中，形成一个原子操作。多条消息要么都发送成功，要么都发送失败。
        		ii) read-process-write模式：将消息消费和生产封装在一个事务中，形成一个原子操作。在一个流式处理的应用中，常常一个服务需要从上游接收消息，然后经过处理后送达到下游，这就对应着消息的消费和生成。

        		kafak在0.11版本开始提供事务支持，提供的是read committed隔离级别的事务。
        		kafka事务性主要是为了解决幂等性无法跨Partition运作的问题，事务性提供了多个Partition写入的原子性，即写入多个Partition 要么全部成功，要么全部失败。

           	 2、事务原理

            	Kafka 0.11.0.0引入了一个服务器端的模块，名为Transaction Coordinator，用于管理Producer发送的消息的事务性。
        		该Transaction Coordinator维护Transaction Log，该log存于一个内部的Topic内。由于Topic数据具有持久性，因此事务的状态也具有持久性。
        		Producer并不直接读写Transaction Log，它与Transaction Coordinator通信，然后由Transaction Coordinator将该事务的状态插入相应的Transaction Log。
        		Transaction Log的设计与Offset Log用于保存Consumer的Offset类似。


     1.6  kafka 如果如何避免消息丢失
        kafka 数据丢失可能发生在broker,producer,consumer三个端

        	1、broker
        		Broker丢失消息是由于Kafka本身的原因造成的，kafka为了得到更高的性能和吞吐量，将数据异步批量的存储在磁盘中。消息的刷盘过程，为了提高性能，减少刷盘次数，kafka采用了批量刷盘的做法。即，按照一定的消息量，和时间间隔进行刷盘。
        		这种机制也是由于linux操作系统决定的。将数据存储到linux操作系统种，会先存储到页缓存（Page cache）中，按照时间或者其他条件进行刷盘（从page cache到file），或者通过fsync命令强制刷盘。数据在page cache中时，如果系统挂掉，数据会丢失。

        		解决方案：
        		kafka通过producer和broker协同处理单个broker丢失参数的情况。一旦producer发现broker消息丢失，即可自动进行retry。除非retry次数超过阀值（可配置），消息才会丢失。此时需要生产者客户端手动处理该情况

        		通过设置 acks 参数 （详解见acks知识）
        	2、	另外producer
        	为了提升效率，减少IO，producer在发送数据时可以将多个请求进行合并后发送。被合并的请求咋发送一线缓存在本地buffer中。缓存的方式和前文提到的刷盘类似，producer可以将请求打包成“块”或者按照时间间隔，将buffer中的数据发出。通过buffer我们可以将生产者改造为异步的方式，而这可以提升我们的发送效率。

        	但是，buffer中的数据就是危险的。在正常情况下，客户端的异步调用可以通过callback来处理消息发送失败或者超时的情况，但是，一旦producer被非法的停止了，那么buffer中的数据将丢失，broker将无法收到该部分数据。又或者，当Producer客户端内存不够时，如果采取的策略是丢弃消息（另一种策略是block阻塞），消息也会被丢失。抑或，消息产生（异步产生）过快，导致挂起线程过多，内存不足，导致程序崩溃，消息丢失



         3、客户端丢失 消息
         	1、Consumer的消费方式主要分为两种：
        		i）  自动提交offset，Automatic Offset Committing
        		ii) 手动提交offset，Manual Offset Control
        	Consumer自动提交的机制是根据一定的时间间隔，将收到的消息进行commit。commit过程和消费消息的过程是异步的。也就是说，可能存在消费过程未成功（比如抛出异常），commit消息已经提交了。此时消息就丢失了。

        	可以保证消息“至少被消费一次”(at least once)。但此时可能出现重复消费的情况

        	1、kafka消息丢失情况
        		1）生产者 和 broker 丢失消息的情况
        		  i） ack = 0  不重试（producer 发送消息完 ，不管结果，如果发送失败也就丢失了）
        		  ii) ack =1 ,leader crash
        		  	producer发送消息完，只等待leader写入成功就返回，leader crash 了，这时 follower还没来的及同步，消息丢失。
        	2、生产者和broke阶段消息丢失解决方案
        		1） 配置ack = all /-1  retries > 1,unclean.leader.election.enable = false;
        			producer 发送消息完，等待follower同步完再返回，如果异常重试，这时副本的数量可能影响吞吐量，最大不超过5个，
        			一般三个就足够了，另外，不允许选举ISR以外的副本作为leader.
        		2) 配置 min.insync.replicas > 1

        			配合 producer acks = -1/all 时使用。min.insync 副本指定必须确认操作成功的最小副本数量，如果 不能满足这个最小值，则生产者将引发异常。
        		3 ) 失败的offset 单独记录
        			producer 发送消息，会自动重试，遇到不可回复异常抛出，这时 可以捕获异常记录到数据库缓存，单独处理。

    补充：
        acks: 生产者客户端根据这个参数来指定分区中必须要有多少个副本收到这条消息之后，生产者才会认为这条消息是写入成功的。

        	1、acks = 1 默认值为1 ，生产者发送消息之后，只要分区的leader副本成功写入消息，那么它就会收到来自服务端的成功响应。

        		如果消息无法写入leader副本，比如在leader副本崩溃，重新选举新的leader副本的过程中，那么生产者就会收到一个错误的响应
        		，为了避免消息丢失，生产者可以重发消息。如果消息写入leader副本并返回成功响应给生产者，且在被其他follower副本拉取之前
        		leader副本崩溃，那么此时消息还是会丢失，因为新选举的leader副本并没有这条对应的消息。

        	2、acks = 0 生产者发送消息之后不需要等待任何服务端的响应。如果在消息从发送到写入kafka的过程中出现某些异常，导致kafak没有收到这条消息，
        	那么生产者也无从得知，消息也就丢失了。

        	3、acks =-1 或 acks = all
        	生产者在消息发送之后，需要等待ISR中的所有副本都成功写入消息之后才能够接收来自服务器端的成功响应。

        	详细：
        	acks=-1 leader broker收到消息后，挂起，等待所有ISR列表中的follower返回结果后，再返回ack。-1等效与all。这种配置下，只有leader写入数据到pagecache是不会返回ack的，还需要所有的ISR返回“成功”才会触发ack。如果此时断电，producer可以知道消息没有被发送成功，将会重新发送。如果在follower收到数据以后，成功返回ack，leader断电，数据将存在于原来的follower中。在重新选举以后，新的leader会持有该部分数据。

        	数据从leader同步到follower，需要2步：
        		数据从pageCache被刷盘到disk。因为只有disk中的数据才能被同步到replica。
        		数据同步到replica，并且replica成功将数据写入PageCache。在producer得到ack后，哪怕是所有机器都停电，数据也至少会存在于leader的磁盘内

        	说明：ISR的列表的follower，需要配合另一个参数才能更好的保证ack的有效性。ISR是Broker维护的一个“可靠的follower列表”，in-sync Replica列表，broker的配置包含一个参数：min.insync.replicas。
        	该参数表示ISR中最少的副本数。如果不设置该值，ISR中的follower列表可能为空。此时相当于acks=1。

   1.7  kafka 如果如何避免消息重复

        3、生产者阶段重复场景

        		1、根本原因
        			生产发送的消息没有收到正确的broke响应，导致producer重试。
        			producer发出一条消息，broke落盘以后因为网络等种种原因发送端得到一个发送失败的响应或者网络中断，然后producer收到一个可恢复的Exception重试消息导致消息重复。

        		2、生产者发送重复解决方案
        			1、启动kafka的幂等性
        			2、ack=0，不重试。（可能会丢消息，适用于吞吐量指标重要性高于数据丢失，例如：日志收集）

        	 4、消费者数据重复场景及解决方案

        	 	1、根本原因
        	 		数据消费完没有及时提交offset到broker
        	 	2、场景
        	 		消息消费端在消费过程中挂掉没有及时提交offset到broke，另一个消费端启动拿之前记录的offset开始消费，由于offset的滞后性可能会导致新启动的客户端有少量重复消费
        	 	3、解决方案
        	 		1、每次消费完或者程序退出时手动提交。这可能也没法保证一条重复。

        	 		2、下游做幂等 （落表： 主键或者唯一索引的方式，避免重复数据）
        				一般的解决方案是让下游做幂等或者尽量每消费一条消息都记录offset，对于少数严格的场景可能需要把offset或唯一ID,例如订单ID和下游状态更新放在同一个数据库里面做事务来保证精确的一次更新或
    1.8 kafak 保证消息顺序

	1、Kafka保证的是分区有序而不是主题有序
	 kafaka 同一主题下的分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息被追加到分区日志文件的时候都会分配一个特定的偏移量（offset）.offset是消息在分区中的唯一标示，Kafak通过它来保证消息在分区内的顺序性。

    个人建议：
 	业务上把需要有序的打到同一个partition，也是一种思路，而且广泛使用。因为大多数情况只需要业务上保证有序就可以，不用全局有。
	partition内部的数据有效性（追加写、offset读）；为了提高Topic的并发吞吐能力，可以提高Topic的partition数，并通过设置partition的replica来保证数据高可靠；但是在多个Partition时，不能保证Topic级别的数据有序性。
    如果你们就像死磕kafka，但是对数据有序性有严格要求，那我建议：创建Topic只指定1个partition，这样的坏处就是磨灭了kafka最优秀的特性。所以可以思考下是不是技术选型有问题， kafka本身适合与流式大数据量，要求高吞吐，对数据有序性要求不严格的场景。。

    3、nsq goland 实现消息队列 （pub/sub模式（发布/订阅，典型的生产者/消费者模式））

    	简介： NSQ是Go语言编写的，开源的分布式消息队列中间件，其设计的目的是用来大规模地处理每天数以十亿计级别的消息。NSQ 具有分布式和去中心化拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保证消息的可靠传递的特征，是一个成熟的、已在大规模生成环境下应用的产品。

    	1、安装和部署：

    	mac :brew instal nsq
    	部署：
    		 1、首先启动nsqlookud
    		   nsqlookupd
    		 2、启动nsqd，并接入刚刚启动的nsqlookud。这里为了方便接下来的测试，启动了两个nsqd
    			nsqd --lookupd-tcp-address=127.0.0.1:4160
    			启动nqsadmin
    			nsqadmin --lookupd-http-address=127.0.0.1:4161

    	浏览器访问： 127.0.0.1:4171。nsq 后台管理

       2、服务介绍
       	1、nsqlookupd 守护进程
       		主要负责服务发现，负责nsqd的心跳，状态检测，给客户端，nsqadmin 提供nsqd地址与状态
       	2、nsqd  守护进程
       		负责接收消息，存储队列和将消息发送给客户

       	3、 nsqadmin nsqadmin是一个web管理界面


      核心概念：

      	topic: 	topic 是nsq的消息发布逻辑关键词， message 属于某个特定的topic，由生产者在发布消息时生成。
      	channels:
      	 	生产者和消费者之间的消息通道，相当于消息队列。

      	当生产者每次发布消息的时候，消息会采用多播的方式被拷贝到各个channel中，channel起到队列的作用


      	message:数据流的形式


      	nsq : 支持延迟消息

      		nsq延时消息队列 使用最小堆算法完成 pqueque最小堆优先队列 （根据时间戳，时间戳最小的放在最前面）




    总结 1、nsq 和 kafka 区别

    	1、如何让一个topic的消息，能够被多个消费者实例消费

    		在nsq中，采用channel的方式，而kafka 引入了消费者组 概念

    	2、如何让mq、生产者、消费者能够自动发现对方
    		这需要一个类似于注册中心的中间件，nsq用的是nsqlookup，而kafka则直接采用了zookeeper:

    	3、 内存 vs 磁盘

    			Nsq 把消息存放在内存，只有当队列里的消息数量超过 --mem-queue-size配置的限制时，才会对消息进行持久化

    	4、无序和有序

    		Nsq 不支持消息顺序消费，
    		kafka 支持消息顺序消费

    	5、Nsq 采用pub/sub 模式，消费者被动接受消息，kafka可以主动选择消费类型


    	6、nsq 支持延时消息投递， nsq 延时消息的实现，采用的是最小堆算法实现，kafka 是根据时间轮实现。

     四、
     服务注册发现

     	概念： 服务发现  就是想要了解集群中是否有进程在监听udp或者tcp端口，并且通过名字进行查找和链接

     	服务发现需要实现的基本功能：

     		1、服务注册： 同一service的所有节点注册到相同的目录下，节点启动后将自己的信息注册到所属的服务的目录中。
     		2、健康检查：服务节点定时进行健康检查。注册到服务目录中的信息设置一个较短的TTL，运行正常的服务节点每隔一段时间
     		去更新信息的TTL，从而达到健康检查效果。
     		3、服务发现： 通过服务节点能查询到服务提供外部访问的IP和端口号。比如网关代理服务时能够及时发现服务中的新增节点、
     		丢弃不可用的服务节点。

     		1、服务注册，就是将提供某个服务的模块信息(通常是这个服务的ip和端口)注册到1个公共的组件上去（比如: zookeeper\consul）。

            2、服务发现，就是新注册的这个服务模块能够及时的被其他调用者发现。不管是服务新增和服务删减都能实现自动发现。

     	1、etcd（go 服务集群）

     		概念： 一个用于配置共享和服务发现的键值存储系统。

     		应用场景：
     		1、服务发现
     			服务发现（Service Discovery）要解决的是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务如何才能找到对方并建立连接。从本质上说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以进行查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。

    		·一个强一致性、高可用的服务存储目录。基于Raft算法的etcd天生就是这样一个强一致性高可用的服务存储目录。

    		·一种注册服务和监控服务健康状态的机制。用户可以在etcd中注册服务，并且对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果。

    		·一种查找和连接服务的机制。通过在etcd指定的主题下注册的服务也能在对应的主题下查找到。为了确保连接，我们可以在每个服务机器上都部署一个proxy模式的etcd，这样就可以确保能访问etcd集群的服务都能互相连接。


     		2、分布式锁

     		因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。

    		1、保持独占，即所有试图获取锁的用户最终只有一个可以得到。etcd为此提供了一套实现分布式锁原子操作CAS（CompareAndSwap）的API。通过设置prevExist值，可以保证在多个节点同时创建某个目录时，只有一个成功，而该用户即可认为是获得了锁。
    		etcdsync 中使用的就是该方式

    		2、控制时序，即所有试图获取锁的用户都会进入等待队列，获得锁的顺序是全局唯一的，同时决定了队列执行顺序。etcd为此也提供了一套API（自动创建有序键），对一个目录建值时指定为POST动作，这样etcd会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用API按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。


     	2、Consul (弹幕专用)  支持Http\gRPC\DNS 多种访问方式，并且保持了CAP中的CP，保持了强一致性和分区容错性
     		也是基于raft算法。
     		的节点中分为 Server 和 Client 两种节点（所有的节点也被称为Agent），Server 节点保存数据，Client 节点负责健康检查及转发数据请求到 Server；Server 节点有一个 Leader 节点和多个 Follower 节点，Leader 节点会将数据同步到 Follower 节点，在 Leader 节点挂掉的时候会启动选举机制产生一个新的 Leader。

     		也是基于 RAFT算法实现

     	3、zookeeper （忽略 zab paxos算法）

     	4、raft 算法

     		参考 https://zhuanlan.zhihu.com/p/32052223

    五、RPC
    RPC 基本概念
    	1、RPC(Remote Procdure Call)

    	RPC(Remote Procedure Call)，远程过程调用，大部分的RPC框架都遵循如下三个开发步骤：
    	1. 定义一个接口说明文件：描述了对象(结构体)、对象成员、接口方法等一系列信息；
    	2. 通过RPC框架所提供的编译器，将接口说明文件编译成具体的语言文件；
    	3. 在客户端和服务器端分别引入RPC编译器所生成的文件，即可像调用本地方法一样调用服务端代码；

    	rpc 通信流程如图：
    		https://img-blog.csdn.net/20170207141803075?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGloYW8yMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast

    	通信过程包括以下几个步骤：
    		1、客户过程以正常方式调用客户桩（client stub，一段代码）；
    		2、客户桩生成一个消息，然后调用本地操作系统；
    		3、客户端操作系统将消息发送给远程操作系统；
    		4、远程操作系统将消息交给服务器桩（server stub，一段代码）；
    		5、服务器桩将参数提取出来，然后调用服务器过程；
    		6、服务器执行要求的操作，操作完成后将结果返回给服务器桩；
    		7、服务器桩将结果打包成一个消息，然后调用本地操作系统；
    		8、服务器操作系统将含有结果的消息发送回客户端操作系统；
    		9、客户端操作系统将消息交给客户桩；
    		10、客户桩将结果从从消息中提取出来，返回给调用它的客户过程；

    	1、Thrift
    		thrift主要用于各个服务之间的RPC通信，支持跨语言。thrift是一个典型的CS结构，客户端和服务端可以使用不同的语言开发，thrift通过IDL(Interface Description Language)来关联客户端和服务端

    		thrift通信流程：
    			Thrift实际上是实现了C/S模式，通过代码生成工具将thrift文生成服务器端和客户端代码（可以为不同语言），从而实现服务端和客户端跨语言的支持。用户在Thirft文件中声明自己的服务，这些服务经过编译后会生成相应语言的代码文件，然后客户端调用服务，服务器端提服务便可以了。


    	 1.1 thrift 网络通信

    	 	thrift的网路通信是自己开发实现的，架构图如下：
    	 	如图：https://images2018.cnblogs.com/blog/645085/201803/645085-20180304155305568-1935823842.png

    	 	协议栈的其他模块都是Thrift的运行时模块：

    			底层IO模块，负责实际的数据传输，包括Socket，文件，或者压缩数据流等。

    			TTransport负责以字节流方式发送和接收Message，是底层IO模块在Thrift框架中的实现，每一个底层IO模块都会有一个对应TTransport来负责Thrift的字节流(Byte Stream)数据在该IO模块上的传输。例如TSocket对应Socket传输，TFileTransport对应文件传输。

    			TProtocol主要负责结构化数据组装成Message，或者从Message结构中读出结构化数据。TProtocol将一个有类型的数据转化为字节流以交给TTransport进行传输，或者从TTransport中读取一定长度的字节数据转化为特定类型的数据。如int32会被TBinaryProtocol Encode为一个四字节的字节数据，或者TBinaryProtocol从TTransport中取出四个字节的数据Decode为int32。

    			TServer负责接收Client的请求，并将请求转发到Processor进行处理。TServer主要任务就是高效的接受Client的请求，特别是在高并发请求的情况下快速完成请求。

    			Processor(或者TProcessor)负责对Client的请求做出相应，包括RPC请求转发，调用参数解析和用户逻辑调用，返回值写回等处理步骤。Processor是服务器端从Thrift框架转入用户逻辑的关键流程。Processor同时也负责向Message结构中写入数据或者读出数据。

    	 	1、TProtocol（协议层），定义数据传输格式，例如：
    		TBinaryProtocol：二进制格式；
    		TCompactProtocol：压缩格式；
    		TJSONProtocol：JSON格式；
    		TSimpleJSONProtocol：提供JSON只写协议, 生成的文件很容易通过脚本语言解析；
    		TDebugProtocol：使用易懂的可读的文本格式，以便于debug

    		2、TTransport（传输层），定义数据传输方式，可以为TCP/IP传输，内存共享或者文件共享等）被用作运行时库。
    		TSocket：阻塞式socker；
    		TFramedTransport：以frame为单位进行传输，非阻塞式服务中使用；
    		TFileTransport：以文件形式进行传输；
    		TMemoryTransport：将内存用于I/O，java实现时内部实际使用了简单的ByteArrayOutputStream；
    		TZlibTransport：使用zlib进行压缩， 与其他传输方式联合使用，当前无java实现；

    		3、Thrift支持的服务模型

    		TSimpleServer
    			简单的单线程服务模型，常用于测试。只在一个单独的线程中以阻塞I/O的方式来提供服务。所以它只能服务一个客户端连接，其他所有客户端在被服务器端接受之前都只能等待。
    		TNonblockingServer
    			它使用了非阻塞式I/O，使用了java.nio.channels.Selector，通过调用select()，它使得程序阻塞在多个连接上，而不是单一的一个连接上。TNonblockingServer处理这些连接的时候，要么接受它，要么从它那读数据，要么把数据写到它那里，然后再次调用select()来等待下一个准备好的可用的连接。通用这种方式，server可同时服务多个客户端，而不会出现一个客户端把其他客户端全部“饿死”的情况。缺点是所有消息是被调用select()方法的同一个线程处理的，服务端同一时间只会处理一个	消息，并没有实现并行处理。
    		THsHaServer（半同步半异步server）
    			针对TNonblockingServer存在的问题，THsHaServer应运而生。它使用一个单独的线程专门负责I/O，同样使用java.nio.channels.Selector，通过调用select()。然后再利用一个独立的worker线程池来处理消息。只要有空闲的worker线程，消息就会被立即处理，因此多条消息能被并行处理。效率进一步得到了提高。
    		TThreadedSelectorServer
    			它与THsHaServer的主要区别在于，TThreadedSelectorServer允许你用多个线程来处理网络I/O。它维护了两个线程池，一个用来处理网络I/O，另一个用来进行请求的处理。
    		TThreadPoolServer
    			它使用的是一种多线程服务模型，使用标准的阻塞式I/O。它会使用一个单独的线程来接收连接。一旦接受了一个连接，它就会被放入ThreadPoolExecutor中的一个worker线程里处理。worker线程被绑定到特定的客户端连接上，直到它关闭。一旦连接关闭，该worker线程就又回到了线程池中。
    			这意味着，如果有1万个并发的客户端连接，你就需要运行1万个线程。所以它对系统资源的消耗不像其他类型的server一样那么“友好”。此外，如果客户端数量超过了线程池中的最大线程数，在有一个worker线程可用之前，请求将被一直阻塞在那里。
    			如果提前知道了将要连接到服务器上的客户端数量，并且不介意运行大量线程的话，TThreadPoolServer可能是个很好的选择。

    	 4、thrift 如何实现多语言通信
    	 	1、数据传输使用socket(多种语言均支持)，数据在以特定的格式(String等)发送，接收方语言进行解析
    	 	2、定义thrift的文件，由thrift文件(IDL)生成双方语言的接口、model，在生成的model以及接口中会有解码编码的代码

     2、grpc
            grpc 是一个高性能，开源和通用的rpc框架，面向移动和HTTP/2设计。

            		调用模型

            		1、客户端（gRPC Stub）调用 A 方法，发起 RPC 调用。

            		2、对请求信息使用 Protobuf 进行对象序列化压缩（IDL）。

            		3、服务端（gRPC Server）接收到请求后，解码请求体，进行业务逻辑处理并返回。

            		4、对响应结果使用 Protobuf 进行对象序列化压缩（IDL）。

            		5、客户端接受到服务端响应，解码请求体。回调被调用的 A 方法，唤醒正在等待响应（阻塞）的客户端调用并返回响应结果。


            		Protocol Buffer 3 :是一种类似XML但更灵活和高效的结构化数据存储格式。
            		protobuf会对proto协议文件进行序列化，最终转换成二进制数据
            		优点： 快：编解码基本都是位运算，也没有复杂的嵌套关系，速度快。


            		Http2协议详解：（HTTP/2（超文本传输协议第2版，最初命名为HTTP2.0），是HTTP协议的第二个主要版本）

            		HTTP1.x的缺点：
            		1、HTTP/1.0一次只允许在一个TCP连接上发起一个请求，HTTP/1.1使用的流水线技术也只能部分处理请求并发，仍然会存在队列头阻塞问题，因此客户端在需要发起多次请求时，通常会采用建立多连接来减少延迟。
            		即（在 HTTP/1.1 协议中 「浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制。超过限制数目的请求会被阻塞」）
            		2、单向请求，只能由客户端发起。
            		3、请求报文与响应报文首部信息冗余量大。
            		4、数据未压缩，导致数据的传输量大。

            		HTTP2.0特点：

            		1、多路复用
            			多路复用允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。

            		2、二进制分帧传
            			在HTTP1.x中，我们是通过文本的方式传输数据，而HTTP2.中 在 应用层(HTTP/2)和传输层(TCP or UDP)之间增加一个二进制分帧层，采用二进制传输。其中HTTP1.x的首部信息会被封装到Headers帧，而Request Body则封装到Data帧。

            		3、服务器Push
            			在HTTP2.0中，服务端可以在客户端某个请求后，主动推送其他资源。


            源码阅读参考https://www.bookstack.cn/read/grpc-read/12-grpc%20%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC.md

     3、自己动手实现geerpc(github)




数据库：

MySql （重点）

NoSQL

1、redis （重点）

2、memecache

3、monodb

4、clickhouse

