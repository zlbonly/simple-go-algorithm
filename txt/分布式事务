分布式事务


分布式：一个业务拆分成多个子业务，每个子业务分别部署在不同的服务器上。每个子系统被称为服务。这些子系统独立运行，之间通过rpc/http进行通信
微服务：微服务的设计是为了不因为某个模块的升级和Bug影响现有的业务系统，即将模块功能进行拆分。


分布式：分散压力
微服务：分散能力

分布式一定是微服务，微服务不一定是分布式。
分布式的定义：把一个服务拆分成多个子服务，分别放在不同的服务器上。微服务可以放在同一个服务器上，也可以放在不同的服务器上。


集群：同一个业务，部署在多个服务器上


分布式 主要为了解决 高并发问题，
集群主要为了解决 高可用问题。





一、
1、分布式ID（雪花算法） 参考： https://github.com/zlbonly/simple-go-algorithm/tree/master/snowflake2/uniqueid
2、分布式锁（redis/zookper/etcd）
3、分布式事务
4、分布式缓存(参考 https://github.com/zlbonly/geecache )


二、 分布式锁

	2.1  基于redis的 setnx实现分布式锁
		基于Redis 实现分布式锁 (setnx) setnx也可以存入key,如果存入key成功则返回1，如果存入的key已经存在了，返回0.


	缺点： 自认为 只适用单实例的redis分布式锁，如果存在redis集群（主从集群，cluster集群）等，可能会存在问题（例如，master 突然挂掉，缓存key 还没有来得及同步到slave）,redis官方适用redlock算法来解决。参考java 封装的redission

	redlock算法 大致：

	公司量级不够，目前redis仅一台服务器，单实例配置，只有需要同步数据的才 进行主从配置。（一主多从）

	2.2、基于Zookerper 实现分布式锁


	2.3、基于etcd

	4.4、数据库乐观锁 (给数据库表添加相应的version版本控制)




	2.2 redis实现分布式锁
	 2.2.1 分布式锁实现需要满足的条件
	  	 1. 互斥性。在任意时刻，只有一个客户端能够持有锁。
	  	 2。不会发生死锁。即时有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证其他客户端能加锁。
	  	 3.具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁
	  	 4。加锁和解锁必须是同一个客户端。客户端自己不能吧别人加的锁给解锁了。


	 正确实现1：使用多参数的set() 使用redis.set(key,value,NX,px,expire_time);
	 错误实现2: 使用redis.setnx()命令加锁，人后使用expire()方法设置过期时间。
		eg: if(redis.setnx(key)){
					reids.expire(key,expireTime);
				}

			问题：setnx（）和expire（）是两条Redis命令，不具备原子性，如果程序在执行setnx（）之后突然崩溃，会导致
			锁没有设置过期时间。将会发生死锁。网上有人这么实现，因为低版本的redis不支持多参数的set()

	错误实现3:
			 实现思路：使用jedis.setnx()命令实现加锁，其中key是锁，value是锁的过期时间。执行过程：1. 通过setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。2. 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功

			 存在问题：
			 	1. 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 2. 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。3. 锁不具备拥有者标识，即任何客户端都可以解锁。
	 正确实现4:

	 		   String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";

        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        使用简单的lua脚本，然后将Lua代码传递到reids.eval()方法中，通过参数赋值相应的lockKey。eval()方法是将Lua代码交给Redis服务端执行，redis服务端可以确保eval()方法的原子性。


        以上策略都是单机Redis实现分布式锁，如果redis集群Master-slave会存在安全性问题：
        	例如：
        		 1、客户端1 从Master获取了锁
        		 2、Master宕机了，存储锁的key还没有来得及同步到Slave上
        		 3、Slave升级为Master
        		 4、客户端2从新的Master获取到了对应同一个资源的锁
        		 于是，客户端1和客户端2同时持有了同一个资源的锁，锁的安全型被打破


        针对redis集群多服务实例场景，可以使用Redlock算法
        可以参考Redisson实现分布式锁，Redis官方提供的分布式锁组件，内部使用了ReadLock算法。
       go的redsync包实现了redlock算法。（https://github.com/go-redsync/redsync）

        	ReadLock红锁算法：
        	1、获得当前时间（ms）

			2、首先设置一个锁有效时间valid_time，也就是超过这个时间后锁自动释放，使用相同的key和value对所有redis实例进行设置，每次链接redis实例时设置一个小于valid_time的超时时间，比如valid_time时10s，那超时时间可以设置成50ms，如果这个实例不行，那么换下一个设置

			3、计算获取锁总共占用的时间，再加上时钟偏移，如果这个总时间小于valid_time，并且成功设置锁的实例数>= N/2 + 1，那么加锁成功

			4、如果加锁成功了，那么这个锁的有效时间就是valid_time - 获取锁占用的时间 - 时钟偏移

			5、如果加锁失败，解锁所有实例（每个redis实例都运行del key）


三、分布式事务

	2.3 分布式事务 基础理论

		2.3.1 事务
		事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。事务应该具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性。

		2.3.2 分布式事务

			分布式事务是指事务的参与者，支持事务的服务器，资源服务器及事务管理器分别位于不同的分布式系统的不同节点上。
			例如：下单接口会扣减库存，生成订单，支付状态等，不仅仅取决于本地的db操作，而且依赖第三方系统的结果。
			这时候分布式事务就保证这些操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。

		2.3.3 CAP原则
			CAP 原则又称 CAP 定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。

			一致性C：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

			可用性（A）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

			分区容错性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区			  的情况，必须就当前操作在 C 和 A 之间做出选择。



		2.3.2 分布式事务使用场景：
		1、转账：
			转账是最经典那的分布式事务场景，假设用户 A 使用银行 app 发起一笔跨行转账给用户 B，银行系统首先扣掉用户 A 的钱，然后增加用户 B 账户中的余额。此时就会出现 2 种异常情况：1. 用户 A 的账户扣款成功，用户 B 账户余额增加失败 2. 用户 A 账户扣款失败，用户 B 账户余额增加成功。对于银行系统来说，以上 2 种情况都是不允许发生，此时就需要分布式事务来保证转账操作的成功。
		2、下单扣库存
			在电商系统中，下单是用户最常见操作。在下单接口中必定会涉及生成订单 id, 扣减库存等操作，对于微服务架构系统，订单 id 与库存服务一般都是独立的服务，此时就需要分布式事务来保证整个下单接口的成功。


		2.3.3 分布式事务解决方案

			1、两段式提交/XA

				两阶段提交，顾名思义就是要分两步提交。存在一个负责协调各个本地资源管理器的事务管理器，本地资源管理器一般是由数据库实现，事务管理器在第一阶段的时候询问各个资源管理器是否都就绪？如果收到每个资源的回复都是 yes，则在第二阶段提交事务，如果其中任意一个资源的回复是 no, 则回滚事务。


				大致流程：
					第一阶段（prepare）：事务管理器向所有本地资源管理器发起请求，询问是否是 ready 状态，所有参与者都将本事务能否成功的信息反馈发给协调者；
					第二阶段 (commit/rollback)：事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。
				存在问题：
					1、同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。

					2、单点故障：一旦事务管理器出现故障，整个系统不可用

					3、数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。

			2、TCC 三段式提交 （try-confirm-cancel）

					TCC(Try Confirm Cancel)
				1、Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性）
                2、Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试。
                 3、Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源 Cancel 操作满足幂等性 Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致。

				在 Try 阶段，是对业务系统进行检查及资源预览，比如订单和存储操作，需要检查库存剩余数量是否够用，并进行预留，预留操作的话就是新建一个可用库存数量字段，Try 阶段操作是对这个可用库存数量进行操作。
				基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现复杂度相对较高。

			3、本地消息表

			4、可靠消息一致性 （大致是使用MQ）

				1、A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作
				2、如果消息发送成功，则执行本地事务
				3、如果本地事务执行成功，则想 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息
				4、B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 		系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求
				5、mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息


			该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与余容错方案基本一致。目前市面上实现该方案的只有阿里的 RocketMq


			5、尽最大努力通知
				尽最大努力通知是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。

				这个方案的大致意思就是：

				系统 A 本地事务执行完之后，发送个消息到 MQ；
				这里会有个专门消费 MQ 的服务，这个服务会消费 MQ 并调用系统 B 的接口；
				要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃


分布式事务参考：小米团队的总结 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/
总结： 分布式锁是解决并发时资源争抢的问题，分布式事务和本地事务是解决流程化提交问题。





四、分布式缓存实现

4.1 常用的分布式缓存，groupcache /memcache/redis

4.2 自实现分布式缓存（参照groupcache）
	1、LUR 缓存淘汰策略
	2、单机并发缓存
	3、HTTP服务端
	4、一致性哈希
	5、分布式节点
	6、防止缓存击穿
	7、支持protobuf通信


1、Lru缓存策略
1.1、FIFO 先进先出
	队列：队列是一种特殊的线性表，只允许在队头进行删除元素，在队尾进行插入。

1.2、缓存淘汰（失效）算法。FIFO,LFU 和LRU
	概述：
	GeeCache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？

  1.2.1
  	FIFO (First In  First Out) 先进先出
  	先进先出，也就是淘汰缓存中最老（即最早）添加的的记录。FIFO认为，最早添加的记录，其不再被使用的可能性难过比刚添加的可能性大。这种算法的实现比较简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景中，部分记录虽然最早被添加，但是也是最常被访问的，而不得不因为待的时间太长而被淘汰。这类数据会被频繁的添加进缓存中，又被淘汰出去。导致缓存命中吕降低。

  1.2.2
  	LFU (Least Frequently Used) 最少使用
  	最少使用。也就淘汰缓存中访问频率最低的记录。LFU认为，如果数据过去被访问多次，那么将来被访问的频率也更高。LFU的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU算法的命中率是比较高的，但是缺点也比较明显，维护每个记录的访问次数，对内存的消耗是很高的；另外，如果数据的访问模式发生变化，LFU需要较长的时间去适应，也就是说LFU算法受历史数据的影响比较大。例如：某个数据历史上访问次数奇高，但是在某个时间点之后就不再被访问了，但是因为历史访问次数过高，而迟迟不能被淘汰。

  1.2.3
  	LRU (Least Recently Used)
  	最近最少使用。相对于仅考虑时间因素的FIFO 和 仅考虑访问频率的LFU，LRU算法可以认为是相对平衡的一种淘汰算法。LRUR认为
  	如果数据最近被访问过，那么将来被访问的频率也会更高。LRU算法的实现非常简单。维护一个队列，如果某条记录被访问了，则移动到队尾，
 	那么队首则是最近最少访问的数据了，淘汰该记录就行。

  1.2.4 LRU 算法实现

  数据结构：字典 key-value，例如： map[key]value 和 双向链表
  说明： 1、字典map，存储键和值的映射关系。这样可以根据某个键（key）查找对应的值value的复杂度是O(1)，在字典中插入一条记录的复杂		  度也是O(1)
        2、使用双向链表实现队列。将所有的值放到双向链表中，这样，访问到某个值时，将其移动到队尾的时间复杂度是O(1)，在队尾新增一条记录以及删除一条记录的复杂度均为O（1）。


    具体实现参照：(https://github.com/zlbonly/simple-go-algorithm/blob/master/lru/lru.go 和 https://geektutu.com/post/geecache.html）
   	1.2.4.1 数据结构定义

	1.2.4.2 查找
		如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
		c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）

	1.2.4.3  删除
		c.ll.Back() 取到队首节点，从链表中删除。
		delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
		更新当前所用的内存 c.nbytes。
	1.2.4.4 新增
		如果键存在，则更新对应节点的值，并将该节点移到队尾。
		不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
		更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。


	目前常用的分布式缓存有memecache 和 redis


   4、一致性哈希实现

    	一致性哈希算法是啥？为什么要使用一致性哈希算法？这和分布式有什么关系？

    	举例子：

    	1.1 我该访问谁？

   		对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

		假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

		那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

		1.2 节点数量变化了怎么办？

		简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 hash(key) % 10 变成了 hash(key) % 9，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 缓存雪崩

		那如何解决这个问题呢？一致性哈希算法可以。


		算法原理：  参考链接：（https://geektutu.com/post/geecache-day4.html）
		一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

		1、计算节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值，放置在环上。
		2、计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

		也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

		2.2 数据倾斜问题

		如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均。

		为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

		假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

		第一步，计算虚拟节点的 Hash 值，放置在环上。
		第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

		虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

		算法实现（go） 参考：https://github.com/zlbonly/simple-go-algorithm/blob/master/consistent/hash/consistenthash.go



二、MQ 总结
	kafak RocketMQ RabbitMQ  Nsq(go语言实现的消息队列)，延迟队列


1、kafka

	1.1基本术语
	Message（消息）：传递数据的对象，主要由四部分组成，offset(偏移量)，key value，timestamp插入时间， 其中offset和timestamp 在kafka集群中产生，， value/key 在produce发送数据时产生

	Broker(代理者)：kafka集群中的机器/服务被称为broker是一个物理概念

	Topic(主题)：维护kafka的消息类型被称为Topic，是一个逻辑概念

	Partition(分区)： 具体维护Kafka扇消息数据的最小单位。一个Topic可以包含多个分区，

	Producer（生产者）： 负责将数据发送到对应的Kafka对应的Topic的进程

	Consumer（消费者）：负责从Topic获取数据的进程

	Consumer Group（消费者组）： 每个consumer都属于一个特定的group组，一个group组可以包含多个consumer，但是一个组中只会有一个consumer消费数据。



	1.2 kafka 吞吐量，速度快的原因

		1.2.1 顺序读写

			kafka将消息记录 持久化到本地磁盘中，kafka的每一个Partition都是一个文件，在收到消息后Kafka会按顺序把数据追插入到文件末尾。

			说明（一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。）

		1.2.2 Page Cache (页缓存)
			为了优化读写操作性能，Kafka利用了操作系统本身的Page Cache，消息先被写入页缓存，然后由操作系统负责刷盘业务，利用操作系统本身的内存而不是Jvm内存空间，
			好处：
				1、避免了创建Object消耗： 如果使用Java堆，java对象内存消耗比较大，通常是所存储数据的两倍甚至更多
				2、避免GC问题：随着Jvm中数据不断增多，垃圾回收变得更加复杂且缓慢，使用系统缓存就不会存在Gc问题。
			通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。
				3、及时Kafka服务重启，页缓存还是会保持有效，进程内的缓存需要重建。


			Page Cache 介绍
				页缓存是操作系统实现的一个主要的磁盘缓存，以此来减少对磁盘I/O 的操作，具体来说就是 把磁盘中的数据缓存到内存中。具体流程
				读： 当一个进程准备读区磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页(page) 是否在页缓存中，
				如果存在（命中）则直接返回数据，从而避免了对物理磁盘的I/O操作； 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。
				写： 如果一个进程需要将数据写入磁盘，那么操作系统也会先检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入相应的页。被修改的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。

				linux 文件cache 分为两层，一个是 page cache ，另外一个是buffer cache ，每一个page cache 包含若干个buffer cache ,通过buffer cache中的指针 指向磁盘block。


		1.2.3 零拷贝

			linux操作系统 “零拷贝” 机制使用了sendfile()方法，允许操作系统将数据从Page Cache 直接 发送到网络，只需要最后一步的copy操作讲数据复制到NIC缓冲区，这样避免了重新复制。

			当Kafka客户端从服务器读取数据时，如果不使用零拷贝技术，那么大致需要经历这样的一个过程：

				1.操作系统将数据从磁盘上读入到内核空间的读缓冲区中。

				2.应用程序（也就是Kafka）从内核空间的读缓冲区将数据拷贝到用户空间的缓冲区中。

				3.应用程序将数据从用户空间的缓冲区再写回到内核空间的socket缓冲区中。

				4.操作系统将socket缓冲区中的数据拷贝到NIC缓冲区中，

				参考图： https://mmbiz.qpic.cn/mmbiz_jpg/hUzEz6BmcaovNVeibZibG1FrLHSNGBKXcLBxeYFPGTPskVaIk7DAZHn9H2Rf2elgBTqy6uSxsapvaxT3HnNVv1icQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1


				注意： 零拷贝并非指一次拷贝都没有，而是避免了在内核空间和用户空间之间的拷贝。



		1.2.4 分区分段 + 索引

			  Kafka的message是按topic分类存储的，topic中的数据又是按照一个一个的partition即分区存储到不同broker节点。每个partition对应了操作系统上的一个文件夹，partition实际上又是按照segment分段存储的。这也非常符合分布式系统分区分桶的设计思想。

			通过这种分区分段的设计，Kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，Kafka又默认为分段后的数据文件建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。


		1.2.5 批量读写

			Kafka数据读写也是批量的而不是单条的。

			除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多

		1.2.6 批量压缩
			在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。
			1、如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩
			2、Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩
			3、Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议

			kafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partion是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。


    2