分布式事务


分布式：一个业务拆分成多个子业务，每个子业务分别部署在不同的服务器上。每个子系统被称为服务。这些子系统独立运行，之间通过rpc/http进行通信
微服务：微服务的设计是为了不因为某个模块的升级和Bug影响现有的业务系统，即将模块功能进行拆分。


分布式：分散压力
微服务：分散能力

分布式一定是微服务，微服务不一定是分布式。
分布式的定义：把一个服务拆分成多个子服务，分别放在不同的服务器上。微服务可以放在同一个服务器上，也可以放在不同的服务器上。


集群：同一个业务，部署在多个服务器上


分布式 主要为了解决 高并发问题，
集群主要为了解决 高可用问题。





一、
1、分布式ID（雪花算法） 参考： https://github.com/zlbonly/simple-go-algorithm/tree/master/snowflake2/uniqueid
2、分布式锁（redis/zookper/etcd）
3、分布式事务
4、分布式缓存(参考 https://github.com/zlbonly/geecache )


二、 分布式锁

	2.1  基于redis的 setnx实现分布式锁
		基于Redis 实现分布式锁 (setnx) setnx也可以存入key,如果存入key成功则返回1，如果存入的key已经存在了，返回0.


	缺点： 自认为 只适用单实例的redis分布式锁，如果存在redis集群（主从集群，cluster集群）等，可能会存在问题（例如，master 突然挂掉，缓存key 还没有来得及同步到slave）,redis官方适用redlock算法来解决。参考java 封装的redission

	redlock算法 大致：

	公司量级不够，目前redis仅一台服务器，单实例配置，只有需要同步数据的才 进行主从配置。（一主多从）

	2.2、基于Zookerper 实现分布式锁


	2.3、基于etcd

	    1、etcd 实现分布式锁 原理
        	 		1、先检查/lock路径下是否有值，如果有值，说明锁已经被占用了，
        	 		2、如果没有值，写入自己的值。写入成功返回，说明加锁成功，如果写入时节点被其他节点写入过了，那么会导致加锁失败
        	 		，跳到第三步
        	 		3、监视/lock 下的时间，先入zuse
        	 		4、当/lock路径下发生事件时，当前进程被唤醒，检查发生的时间是否是删除事件（说明锁持有着主动解锁） 或者过期事件
        	 		(说明锁过期失效) 如果是的话，那么回到1，走抢锁流程。

        	 		etcd v3 api 官方已经提供了可直接使用的锁 API
        	 	 参考： https://github.com/zieckey/etcdsync/blob/master/mutex.go 实现：

        	 	 demo :使用上，跟 Golang 官方 sync 包的 Mutex 接口非常类似，先New()，然后调用Lock()，使用完后调用Unlock()，

        		 	 func main() {
        		//etcdsync.SetDebug(true)
        		log.SetFlags(log.Ldate|log.Ltime|log.Lshortfile)
        		m := etcdsync.New("/etcdsync", "123", []string{"http://127.0.0.1:2379"})
        		if m == nil {
        		log.Printf("etcdsync.NewMutex failed")
        		}
        		err := m.Lock()
        		if err != nil {
        		log.Printf("etcdsync.Lock failed")
        		} else {
        		log.Printf("etcdsync.Lock OK")
        		}

        		log.Printf("Get the lock. Do something here.")

        		err = m.Unlock()
        		if err != nil {
        		log.Printf("etcdsync.Unlock failed")
        		} else {
        			log.Printf("etcdsync.Unlock OK")
        		}
        	}

	4.4、数据库乐观锁 (给数据库表添加相应的version版本控制)




	2.2 redis实现分布式锁
	 2.2.1 分布式锁实现需要满足的条件
	  	 1. 互斥性。在任意时刻，只有一个客户端能够持有锁。
	  	 2。不会发生死锁。即时有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证其他客户端能加锁。
	  	 3.具有容错性。只要大部分的Redis节点正常运行，客户端就可以加锁和解锁
	  	 4。加锁和解锁必须是同一个客户端。客户端自己不能吧别人加的锁给解锁了。


	 正确实现1：使用多参数的set() 使用redis.set(key,value,NX,px,expire_time);
	 错误实现2: 使用redis.setnx()命令加锁，人后使用expire()方法设置过期时间。
		eg: if(redis.setnx(key)){
					reids.expire(key,expireTime);
				}

			问题：setnx（）和expire（）是两条Redis命令，不具备原子性，如果程序在执行setnx（）之后突然崩溃，会导致
			锁没有设置过期时间。将会发生死锁。网上有人这么实现，因为低版本的redis不支持多参数的set()

	错误实现3:
			 实现思路：使用jedis.setnx()命令实现加锁，其中key是锁，value是锁的过期时间。执行过程：1. 通过setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。2. 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功

			 存在问题：
			 	1. 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 2. 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。3. 锁不具备拥有者标识，即任何客户端都可以解锁。
	 正确实现4:

	 		   String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";

        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        使用简单的lua脚本，然后将Lua代码传递到reids.eval()方法中，通过参数赋值相应的lockKey。eval()方法是将Lua代码交给Redis服务端执行，redis服务端可以确保eval()方法的原子性。


        以上策略都是单机Redis实现分布式锁，如果redis集群Master-slave会存在安全性问题：
        	例如：
        		 1、客户端1 从Master获取了锁
        		 2、Master宕机了，存储锁的key还没有来得及同步到Slave上
        		 3、Slave升级为Master
        		 4、客户端2从新的Master获取到了对应同一个资源的锁
        		 于是，客户端1和客户端2同时持有了同一个资源的锁，锁的安全型被打破


        针对redis集群多服务实例场景，可以使用Redlock算法
        可以参考Redisson实现分布式锁，Redis官方提供的分布式锁组件，内部使用了ReadLock算法。
       go的redsync包实现了redlock算法。（https://github.com/go-redsync/redsync）

        	ReadLock红锁算法：
        	1、获得当前时间（ms）

			2、首先设置一个锁有效时间valid_time，也就是超过这个时间后锁自动释放，使用相同的key和value对所有redis实例进行设置，每次链接redis实例时设置一个小于valid_time的超时时间，比如valid_time时10s，那超时时间可以设置成50ms，如果这个实例不行，那么换下一个设置

			3、计算获取锁总共占用的时间，再加上时钟偏移，如果这个总时间小于valid_time，并且成功设置锁的实例数>= N/2 + 1，那么加锁成功

			4、如果加锁成功了，那么这个锁的有效时间就是valid_time - 获取锁占用的时间 - 时钟偏移

			5、如果加锁失败，解锁所有实例（每个redis实例都运行del key）


三、分布式事务

	2.3 分布式事务 基础理论

		2.3.1 事务
		事务是应用程序中一系列严密的操作，所有操作必须成功完成，否则在每个操作中所作的所有更改都会被撤消。也就是事务具有原子性，一个事务中的一系列的操作要么全部成功，要么一个都不做。事务应该具有 4 个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为 ACID 特性。

		2.3.2 分布式事务

			分布式事务是指事务的参与者，支持事务的服务器，资源服务器及事务管理器分别位于不同的分布式系统的不同节点上。
			例如：下单接口会扣减库存，生成订单，支付状态等，不仅仅取决于本地的db操作，而且依赖第三方系统的结果。
			这时候分布式事务就保证这些操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。

		2.3.3 CAP原则
			CAP 原则又称 CAP 定理，指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。

			一致性C：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

			可用性（A）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）

			分区容错性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区			  的情况，必须就当前操作在 C 和 A 之间做出选择。



		2.3.2 分布式事务使用场景：
		1、转账：
			转账是最经典那的分布式事务场景，假设用户 A 使用银行 app 发起一笔跨行转账给用户 B，银行系统首先扣掉用户 A 的钱，然后增加用户 B 账户中的余额。此时就会出现 2 种异常情况：1. 用户 A 的账户扣款成功，用户 B 账户余额增加失败 2. 用户 A 账户扣款失败，用户 B 账户余额增加成功。对于银行系统来说，以上 2 种情况都是不允许发生，此时就需要分布式事务来保证转账操作的成功。
		2、下单扣库存
			在电商系统中，下单是用户最常见操作。在下单接口中必定会涉及生成订单 id, 扣减库存等操作，对于微服务架构系统，订单 id 与库存服务一般都是独立的服务，此时就需要分布式事务来保证整个下单接口的成功。


		2.3.3 分布式事务解决方案

			1、两段式提交/XA

				两阶段提交，顾名思义就是要分两步提交。存在一个负责协调各个本地资源管理器的事务管理器，本地资源管理器一般是由数据库实现，事务管理器在第一阶段的时候询问各个资源管理器是否都就绪？如果收到每个资源的回复都是 yes，则在第二阶段提交事务，如果其中任意一个资源的回复是 no, 则回滚事务。


				大致流程：
					第一阶段（prepare）：事务管理器向所有本地资源管理器发起请求，询问是否是 ready 状态，所有参与者都将本事务能否成功的信息反馈发给协调者；
					第二阶段 (commit/rollback)：事务管理器根据所有本地资源管理器的反馈，通知所有本地资源管理器，步调一致地在所有分支上提交或者回滚。
				存在问题：
					1、同步阻塞：当参与事务者存在占用公共资源的情况，其中一个占用了资源，其他事务参与者就只能阻塞等待资源释放，处于阻塞状态。

					2、单点故障：一旦事务管理器出现故障，整个系统不可用

					3、数据不一致：在阶段二，如果事务管理器只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。

			2、TCC 三段式提交 （try-confirm-cancel）

					TCC(Try Confirm Cancel)
				1、Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性）
                2、Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作满足幂等性。要求具备幂等设计，Confirm 失败后需要进行重试。
                 3、Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源 Cancel 操作满足幂等性 Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致。

				在 Try 阶段，是对业务系统进行检查及资源预览，比如订单和存储操作，需要检查库存剩余数量是否够用，并进行预留，预留操作的话就是新建一个可用库存数量字段，Try 阶段操作是对这个可用库存数量进行操作。
				基于 TCC 实现分布式事务，会将原来只需要一个接口就可以实现的逻辑拆分为 Try、Confirm、Cancel 三个接口，所以代码实现复杂度相对较高。

			3、本地消息表

			4、可靠消息一致性 （大致是使用MQ）

				1、A 系统先向 mq 发送一条 prepare 消息，如果 prepare 消息发送失败，则直接取消操作
				2、如果消息发送成功，则执行本地事务
				3、如果本地事务执行成功，则想 mq 发送一条 confirm 消息，如果发送失败，则发送回滚消息
				4、B 系统定期消费 mq 中的 confirm 消息，执行本地事务，并发送 ack 消息。如果 B 		系统中的本地事务失败，会一直不断重试，如果是业务失败，会向 A 系统发起回滚请求
				5、mq 会定期轮询所有 prepared 消息调用系统 A 提供的接口查询消息的处理情况，如果该 prepare 消息本地事务处理成功，则重新发送 confirm 消息，否则直接回滚该消息


			该方案与本地消息最大的不同是去掉了本地消息表，其次本地消息表依赖消息表重试写入 mq 这一步由本方案中的轮询 prepare 消息状态来重试或者回滚该消息替代。其实现条件与余容错方案基本一致。目前市面上实现该方案的只有阿里的 RocketMq


			5、尽最大努力通知
				尽最大努力通知是最简单的一种柔性事务，适用于一些最终一致性时间敏感度低的业务，且被动方处理结果 不影响主动方的处理结果。

				这个方案的大致意思就是：

				系统 A 本地事务执行完之后，发送个消息到 MQ；
				这里会有个专门消费 MQ 的服务，这个服务会消费 MQ 并调用系统 B 的接口；
				要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B, 反复 N 次，最后还是不行就放弃


分布式事务参考：小米团队的总结 https://xiaomi-info.github.io/2020/01/02/distributed-transaction/
总结： 分布式锁是解决并发时资源争抢的问题，分布式事务和本地事务是解决流程化提交问题。





四、分布式缓存实现

4.1 常用的分布式缓存，groupcache /memcache/redis

4.2 自实现分布式缓存（参照groupcache）
	1、LUR 缓存淘汰策略
	2、单机并发缓存
	3、HTTP服务端
	4、一致性哈希
	5、分布式节点
	6、防止缓存击穿
	7、支持protobuf通信


1、Lru缓存策略
1.1、FIFO 先进先出
	队列：队列是一种特殊的线性表，只允许在队头进行删除元素，在队尾进行插入。

1.2、缓存淘汰（失效）算法。FIFO,LFU 和LRU
	概述：
	GeeCache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？

  1.2.1
  	FIFO (First In  First Out) 先进先出
  	先进先出，也就是淘汰缓存中最老（即最早）添加的的记录。FIFO认为，最早添加的记录，其不再被使用的可能性难过比刚添加的可能性大。这种算法的实现比较简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景中，部分记录虽然最早被添加，但是也是最常被访问的，而不得不因为待的时间太长而被淘汰。这类数据会被频繁的添加进缓存中，又被淘汰出去。导致缓存命中吕降低。

  1.2.2
  	LFU (Least Frequently Used) 最少使用
  	最少使用。也就淘汰缓存中访问频率最低的记录。LFU认为，如果数据过去被访问多次，那么将来被访问的频率也更高。LFU的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU算法的命中率是比较高的，但是缺点也比较明显，维护每个记录的访问次数，对内存的消耗是很高的；另外，如果数据的访问模式发生变化，LFU需要较长的时间去适应，也就是说LFU算法受历史数据的影响比较大。例如：某个数据历史上访问次数奇高，但是在某个时间点之后就不再被访问了，但是因为历史访问次数过高，而迟迟不能被淘汰。

  1.2.3
  	LRU (Least Recently Used)
  	最近最少使用。相对于仅考虑时间因素的FIFO 和 仅考虑访问频率的LFU，LRU算法可以认为是相对平衡的一种淘汰算法。LRUR认为
  	如果数据最近被访问过，那么将来被访问的频率也会更高。LRU算法的实现非常简单。维护一个队列，如果某条记录被访问了，则移动到队尾，
 	那么队首则是最近最少访问的数据了，淘汰该记录就行。

  1.2.4 LRU 算法实现

  数据结构：字典 key-value，例如： map[key]value 和 双向链表
  说明： 1、字典map，存储键和值的映射关系。这样可以根据某个键（key）查找对应的值value的复杂度是O(1)，在字典中插入一条记录的复杂		  度也是O(1)
        2、使用双向链表实现队列。将所有的值放到双向链表中，这样，访问到某个值时，将其移动到队尾的时间复杂度是O(1)，在队尾新增一条记录以及删除一条记录的复杂度均为O（1）。


    具体实现参照：(https://github.com/zlbonly/simple-go-algorithm/blob/master/lru/lru.go 和 https://geektutu.com/post/geecache.html）
   	1.2.4.1 数据结构定义

	1.2.4.2 查找
		如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
		c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）

	1.2.4.3  删除
		c.ll.Back() 取到队首节点，从链表中删除。
		delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
		更新当前所用的内存 c.nbytes。
	1.2.4.4 新增
		如果键存在，则更新对应节点的值，并将该节点移到队尾。
		不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
		更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。


	目前常用的分布式缓存有memecache 和 redis


   4、一致性哈希实现

    	一致性哈希算法是啥？为什么要使用一致性哈希算法？这和分布式有什么关系？

    	举例子：

    	1.1 我该访问谁？

   		对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

		假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

		那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

		1.2 节点数量变化了怎么办？

		简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 hash(key) % 10 变成了 hash(key) % 9，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 缓存雪崩

		那如何解决这个问题呢？一致性哈希算法可以。


		算法原理：  参考链接：（https://geektutu.com/post/geecache-day4.html）
		一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

		1、计算节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值，放置在环上。
		2、计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

		也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

		2.2 数据倾斜问题

		如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均。

		为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

		假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

		第一步，计算虚拟节点的 Hash 值，放置在环上。
		第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

		虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

		算法实现（go） 参考：https://github.com/zlbonly/simple-go-algorithm/blob/master/consistent/hash/consistenthash.go



二、MQ 总结
	kafak RocketMQ RabbitMQ  Nsq(go语言实现的消息队列)，延迟队列


1、kafka

	1.1基本术语
	Message（消息）：传递数据的对象，主要由四部分组成，offset(偏移量)，key value，timestamp插入时间， 其中offset和timestamp 在kafka集群中产生，， value/key 在produce发送数据时产生

	Broker(代理者)：kafka集群中的机器/服务被称为broker是一个物理概念

	Topic(主题)：维护kafka的消息类型被称为Topic，是一个逻辑概念

	Partition(分区)： 具体维护Kafka扇消息数据的最小单位。一个Topic可以包含多个分区，

	Producer（生产者）： 负责将数据发送到对应的Kafka对应的Topic的进程

	Consumer（消费者）：负责从Topic获取数据的进程

	Consumer Group（消费者组）： 每个consumer都属于一个特定的group组，一个group组可以包含多个consumer，但是一个组中只会有一个consumer消费数据。



	1.2 kafka 吞吐量，速度快的原因

		1.2.1 顺序读写

			kafka将消息记录 持久化到本地磁盘中，kafka的每一个Partition都是一个文件，在收到消息后Kafka会按顺序把数据追插入到文件末尾。

			说明（一般人会认为磁盘读写性能差，可能会对Kafka性能如何保证提出质疑。实际上不管是内存还是磁盘，快或慢关键在于寻址的方式，磁盘分为顺序读写与随机读写，内存也一样分为顺序读写与随机读写。基于磁盘的随机读写确实很慢，但磁盘的顺序读写性能却很高，一般而言要高出磁盘随机读写三个数量级，一些情况下磁盘顺序读写性能甚至要高于内存随机读写。）

		1.2.2 Page Cache (页缓存)
			为了优化读写操作性能，Kafka利用了操作系统本身的Page Cache，消息先被写入页缓存，然后由操作系统负责刷盘业务，利用操作系统本身的内存而不是Jvm内存空间，
			好处：
				1、避免了创建Object消耗： 如果使用Java堆，java对象内存消耗比较大，通常是所存储数据的两倍甚至更多
				2、避免GC问题：随着Jvm中数据不断增多，垃圾回收变得更加复杂且缓慢，使用系统缓存就不会存在Gc问题。
			通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。
				3、及时Kafka服务重启，页缓存还是会保持有效，进程内的缓存需要重建。


			Page Cache 介绍
				页缓存是操作系统实现的一个主要的磁盘缓存，以此来减少对磁盘I/O 的操作，具体来说就是 把磁盘中的数据缓存到内存中。具体流程
				读： 当一个进程准备读区磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页(page) 是否在页缓存中，
				如果存在（命中）则直接返回数据，从而避免了对物理磁盘的I/O操作； 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。
				写： 如果一个进程需要将数据写入磁盘，那么操作系统也会先检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入相应的页。被修改的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。

				linux 文件cache 分为两层，一个是 page cache ，另外一个是buffer cache ，每一个page cache 包含若干个buffer cache ,通过buffer cache中的指针 指向磁盘block。


		1.2.3 零拷贝

			linux操作系统 “零拷贝” 机制使用了sendfile()方法，允许操作系统将数据从Page Cache 直接 发送到网络，只需要最后一步的copy操作讲数据复制到NIC缓冲区，这样避免了重新复制。

			当Kafka客户端从服务器读取数据时，如果不使用零拷贝技术，那么大致需要经历这样的一个过程：

				1.操作系统将数据从磁盘上读入到内核空间的读缓冲区中。

				2.应用程序（也就是Kafka）从内核空间的读缓冲区将数据拷贝到用户空间的缓冲区中。

				3.应用程序将数据从用户空间的缓冲区再写回到内核空间的socket缓冲区中。

				4.操作系统将socket缓冲区中的数据拷贝到NIC缓冲区中，

				参考图： https://mmbiz.qpic.cn/mmbiz_jpg/hUzEz6BmcaovNVeibZibG1FrLHSNGBKXcLBxeYFPGTPskVaIk7DAZHn9H2Rf2elgBTqy6uSxsapvaxT3HnNVv1icQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1


				注意： 零拷贝并非指一次拷贝都没有，而是避免了在内核空间和用户空间之间的拷贝。



		1.2.4 分区分段 + 索引

			  Kafka的message是按topic分类存储的，topic中的数据又是按照一个一个的partition即分区存储到不同broker节点。每个partition对应了操作系统上的一个文件夹，partition实际上又是按照segment分段存储的。这也非常符合分布式系统分区分桶的设计思想。

			通过这种分区分段的设计，Kafka的message消息实际上是分布式存储在一个一个小的segment中的，每次文件操作也是直接操作的segment。为了进一步的查询优化，Kafka又默认为分段后的数据文件建立了索引文件，就是文件系统上的.index文件。这种分区分段+索引的设计，不仅提升了数据读取的效率，同时也提高了数据操作的并行度。


		1.2.5 批量读写

			Kafka数据读写也是批量的而不是单条的。

			除了利用底层的技术外，Kafka还在应用程序层面提供了一些手段来提升性能。最明显的就是使用批次。在向Kafka写入数据时，可以启用批次写入，这样可以避免在网络上频繁传输单个消息带来的延迟和带宽开销。假设网络带宽为10MB/S，一次性传输10MB的消息比传输1KB的消息10000万次显然要快得多

		1.2.6 批量压缩
			在很多情况下，系统的瓶颈不是CPU或磁盘，而是网络IO，进行数据压缩会消耗少量的CPU资源,不过对于kafka而言,网络IO更应该需要考虑。
			1、如果每个消息都压缩，但是压缩率相对很低，所以Kafka使用了批量压缩，即将多个消息一起压缩而不是单个消息压缩
			2、Kafka允许使用递归的消息集合，批量的消息可以通过压缩的形式传输并且在日志中也可以保持压缩格式，直到被消费者解压缩
			3、Kafka支持多种压缩协议，包括Gzip和Snappy压缩协议

			kafka速度的秘诀在于，它把所有的消息都变成一个批量的文件，并且进行合理的批量压缩，减少网络IO损耗，通过mmap提高I/O速度，写入数据的时候由于单个Partion是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。


    3、nsq goland 实现消息队列 （pub/sub模式（发布/订阅，典型的生产者/消费者模式））

    	简介： NSQ是Go语言编写的，开源的分布式消息队列中间件，其设计的目的是用来大规模地处理每天数以十亿计级别的消息。NSQ 具有分布式和去中心化拓扑结构，该结构具有无单点故障、故障容错、高可用性以及能够保证消息的可靠传递的特征，是一个成熟的、已在大规模生成环境下应用的产品。

    	1、安装和部署：

    	mac :brew instal nsq
    	部署：
    		 1、首先启动nsqlookud
    		   nsqlookupd
    		 2、启动nsqd，并接入刚刚启动的nsqlookud。这里为了方便接下来的测试，启动了两个nsqd
    			nsqd --lookupd-tcp-address=127.0.0.1:4160
    			启动nqsadmin
    			nsqadmin --lookupd-http-address=127.0.0.1:4161

    	浏览器访问： 127.0.0.1:4171。nsq 后台管理

       2、服务介绍
       	1、nsqlookupd 守护进程
       		主要负责服务发现，负责nsqd的心跳，状态检测，给客户端，nsqadmin 提供nsqd地址与状态
       	2、nsqd  守护进程
       		负责接收消息，存储队列和将消息发送给客户

       	3、 nsqadmin nsqadmin是一个web管理界面


      核心概念：

      	topic: 	topic 是nsq的消息发布逻辑关键词， message 属于某个特定的topic，由生产者在发布消息时生成。
      	channels:
      	 	生产者和消费者之间的消息通道，相当于消息队列。

      	当生产者每次发布消息的时候，消息会采用多播的方式被拷贝到各个channel中，channel起到队列的作用


      	message:数据流的形式


      	nsq : 支持延迟消息

      		nsq延时消息队列 使用最小堆算法完成 pqueque最小堆优先队列 （根据时间戳，时间戳最小的放在最前面）




    总结 1、nsq 和 kafka 区别

    	1、如何让一个topic的消息，能够被多个消费者实例消费

    		在nsq中，采用channel的方式，而kafka 引入了消费者组 概念

    	2、如何让mq、生产者、消费者能够自动发现对方
    		这需要一个类似于注册中心的中间件，nsq用的是nsqlookup，而kafka则直接采用了zookeeper:

    	3、 内存 vs 磁盘

    			Nsq 把消息存放在内存，只有当队列里的消息数量超过 --mem-queue-size配置的限制时，才会对消息进行持久化

    	4、无序和有序

    		Nsq 不支持消息顺序消费，
    		kafka 支持消息顺序消费

    	5、Nsq 采用pub/sub 模式，消费者被动接受消息，kafka可以主动选择消费类型


    	6、nsq 支持延时消息投递， nsq 延时消息的实现，采用的是最小堆算法实现，kafka 是根据时间轮实现。

     四、
     服务注册发现

     	概念： 服务发现  就是想要了解集群中是否有进程在监听udp或者tcp端口，并且通过名字进行查找和链接

     	服务发现需要实现的基本功能：

     		1、服务注册： 同一service的所有节点注册到相同的目录下，节点启动后将自己的信息注册到所属的服务的目录中。
     		2、健康检查：服务节点定时进行健康检查。注册到服务目录中的信息设置一个较短的TTL，运行正常的服务节点每隔一段时间
     		去更新信息的TTL，从而达到健康检查效果。
     		3、服务发现： 通过服务节点能查询到服务提供外部访问的IP和端口号。比如网关代理服务时能够及时发现服务中的新增节点、
     		丢弃不可用的服务节点。

     	1、etcd（go 服务集群）

     		概念： 一个用于配置共享和服务发现的键值存储系统。

     		应用场景：
     		1、服务发现
     			服务发现（Service Discovery）要解决的是分布式系统中最常见的问题之一，即在同一个分布式集群中的进程或服务如何才能找到对方并建立连接。从本质上说，服务发现就是想要了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以进行查找和连接。要解决服务发现的问题，需要有下面三大支柱，缺一不可。

    		·一个强一致性、高可用的服务存储目录。基于Raft算法的etcd天生就是这样一个强一致性高可用的服务存储目录。

    		·一种注册服务和监控服务健康状态的机制。用户可以在etcd中注册服务，并且对注册的服务设置key TTL，定时保持服务的心跳以达到监控健康状态的效果。

    		·一种查找和连接服务的机制。通过在etcd指定的主题下注册的服务也能在对应的主题下查找到。为了确保连接，我们可以在每个服务机器上都部署一个proxy模式的etcd，这样就可以确保能访问etcd集群的服务都能互相连接。


     		2、分布式锁

     		因为etcd使用Raft算法保持了数据的强一致性，某次操作存储到集群中的值必然是全局一致的，所以很容易实现分布式锁。锁服务有两种使用方式，一是保持独占，二是控制时序。

    		1、保持独占，即所有试图获取锁的用户最终只有一个可以得到。etcd为此提供了一套实现分布式锁原子操作CAS（CompareAndSwap）的API。通过设置prevExist值，可以保证在多个节点同时创建某个目录时，只有一个成功，而该用户即可认为是获得了锁。
    		etcdsync 中使用的就是该方式

    		2、控制时序，即所有试图获取锁的用户都会进入等待队列，获得锁的顺序是全局唯一的，同时决定了队列执行顺序。etcd为此也提供了一套API（自动创建有序键），对一个目录建值时指定为POST动作，这样etcd会自动在目录下生成一个当前最大的值为键，存储这个新的值（客户端编号）。同时还可以使用API按顺序列出所有当前目录下的键值。此时这些键的值就是客户端的时序，而这些键中存储的值可以是代表客户端的编号。


     	2、Consul (弹幕专用)  支持Http\gRPC\DNS 多种访问方式，并且保持了CAP中的CP，保持了强一致性和分区容错性
     		也是基于raft算法。
     		的节点中分为 Server 和 Client 两种节点（所有的节点也被称为Agent），Server 节点保存数据，Client 节点负责健康检查及转发数据请求到 Server；Server 节点有一个 Leader 节点和多个 Follower 节点，Leader 节点会将数据同步到 Follower 节点，在 Leader 节点挂掉的时候会启动选举机制产生一个新的 Leader。

     		也是基于 RAFT算法实现

     	3、zookeeper （忽略 zab paxos算法）

     	4、raft 算法

     		参考 https://zhuanlan.zhihu.com/p/32052223

    五、RPC
    RPC 基本概念
    	1、RPC(Remote Procdure Call)

    	RPC(Remote Procedure Call)，远程过程调用，大部分的RPC框架都遵循如下三个开发步骤：
    	1. 定义一个接口说明文件：描述了对象(结构体)、对象成员、接口方法等一系列信息；
    	2. 通过RPC框架所提供的编译器，将接口说明文件编译成具体的语言文件；
    	3. 在客户端和服务器端分别引入RPC编译器所生成的文件，即可像调用本地方法一样调用服务端代码；

    	rpc 通信流程如图：
    		https://img-blog.csdn.net/20170207141803075?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGloYW8yMQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast

    	通信过程包括以下几个步骤：
    		1、客户过程以正常方式调用客户桩（client stub，一段代码）；
    		2、客户桩生成一个消息，然后调用本地操作系统；
    		3、客户端操作系统将消息发送给远程操作系统；
    		4、远程操作系统将消息交给服务器桩（server stub，一段代码）；
    		5、服务器桩将参数提取出来，然后调用服务器过程；
    		6、服务器执行要求的操作，操作完成后将结果返回给服务器桩；
    		7、服务器桩将结果打包成一个消息，然后调用本地操作系统；
    		8、服务器操作系统将含有结果的消息发送回客户端操作系统；
    		9、客户端操作系统将消息交给客户桩；
    		10、客户桩将结果从从消息中提取出来，返回给调用它的客户过程；

    	1、Thrift
    		thrift主要用于各个服务之间的RPC通信，支持跨语言。thrift是一个典型的CS结构，客户端和服务端可以使用不同的语言开发，thrift通过IDL(Interface Description Language)来关联客户端和服务端

    		thrift通信流程：
    			Thrift实际上是实现了C/S模式，通过代码生成工具将thrift文生成服务器端和客户端代码（可以为不同语言），从而实现服务端和客户端跨语言的支持。用户在Thirft文件中声明自己的服务，这些服务经过编译后会生成相应语言的代码文件，然后客户端调用服务，服务器端提服务便可以了。


    	 1.1 thrift 网络通信

    	 	thrift的网路通信是自己开发实现的，架构图如下：
    	 	如图：https://images2018.cnblogs.com/blog/645085/201803/645085-20180304155305568-1935823842.png

    	 	协议栈的其他模块都是Thrift的运行时模块：

    			底层IO模块，负责实际的数据传输，包括Socket，文件，或者压缩数据流等。

    			TTransport负责以字节流方式发送和接收Message，是底层IO模块在Thrift框架中的实现，每一个底层IO模块都会有一个对应TTransport来负责Thrift的字节流(Byte Stream)数据在该IO模块上的传输。例如TSocket对应Socket传输，TFileTransport对应文件传输。

    			TProtocol主要负责结构化数据组装成Message，或者从Message结构中读出结构化数据。TProtocol将一个有类型的数据转化为字节流以交给TTransport进行传输，或者从TTransport中读取一定长度的字节数据转化为特定类型的数据。如int32会被TBinaryProtocol Encode为一个四字节的字节数据，或者TBinaryProtocol从TTransport中取出四个字节的数据Decode为int32。

    			TServer负责接收Client的请求，并将请求转发到Processor进行处理。TServer主要任务就是高效的接受Client的请求，特别是在高并发请求的情况下快速完成请求。

    			Processor(或者TProcessor)负责对Client的请求做出相应，包括RPC请求转发，调用参数解析和用户逻辑调用，返回值写回等处理步骤。Processor是服务器端从Thrift框架转入用户逻辑的关键流程。Processor同时也负责向Message结构中写入数据或者读出数据。

    	 	1、TProtocol（协议层），定义数据传输格式，例如：
    		TBinaryProtocol：二进制格式；
    		TCompactProtocol：压缩格式；
    		TJSONProtocol：JSON格式；
    		TSimpleJSONProtocol：提供JSON只写协议, 生成的文件很容易通过脚本语言解析；
    		TDebugProtocol：使用易懂的可读的文本格式，以便于debug

    		2、TTransport（传输层），定义数据传输方式，可以为TCP/IP传输，内存共享或者文件共享等）被用作运行时库。
    		TSocket：阻塞式socker；
    		TFramedTransport：以frame为单位进行传输，非阻塞式服务中使用；
    		TFileTransport：以文件形式进行传输；
    		TMemoryTransport：将内存用于I/O，java实现时内部实际使用了简单的ByteArrayOutputStream；
    		TZlibTransport：使用zlib进行压缩， 与其他传输方式联合使用，当前无java实现；

    		3、Thrift支持的服务模型

    		TSimpleServer
    			简单的单线程服务模型，常用于测试。只在一个单独的线程中以阻塞I/O的方式来提供服务。所以它只能服务一个客户端连接，其他所有客户端在被服务器端接受之前都只能等待。
    		TNonblockingServer
    			它使用了非阻塞式I/O，使用了java.nio.channels.Selector，通过调用select()，它使得程序阻塞在多个连接上，而不是单一的一个连接上。TNonblockingServer处理这些连接的时候，要么接受它，要么从它那读数据，要么把数据写到它那里，然后再次调用select()来等待下一个准备好的可用的连接。通用这种方式，server可同时服务多个客户端，而不会出现一个客户端把其他客户端全部“饿死”的情况。缺点是所有消息是被调用select()方法的同一个线程处理的，服务端同一时间只会处理一个	消息，并没有实现并行处理。
    		THsHaServer（半同步半异步server）
    			针对TNonblockingServer存在的问题，THsHaServer应运而生。它使用一个单独的线程专门负责I/O，同样使用java.nio.channels.Selector，通过调用select()。然后再利用一个独立的worker线程池来处理消息。只要有空闲的worker线程，消息就会被立即处理，因此多条消息能被并行处理。效率进一步得到了提高。
    		TThreadedSelectorServer
    			它与THsHaServer的主要区别在于，TThreadedSelectorServer允许你用多个线程来处理网络I/O。它维护了两个线程池，一个用来处理网络I/O，另一个用来进行请求的处理。
    		TThreadPoolServer
    			它使用的是一种多线程服务模型，使用标准的阻塞式I/O。它会使用一个单独的线程来接收连接。一旦接受了一个连接，它就会被放入ThreadPoolExecutor中的一个worker线程里处理。worker线程被绑定到特定的客户端连接上，直到它关闭。一旦连接关闭，该worker线程就又回到了线程池中。
    			这意味着，如果有1万个并发的客户端连接，你就需要运行1万个线程。所以它对系统资源的消耗不像其他类型的server一样那么“友好”。此外，如果客户端数量超过了线程池中的最大线程数，在有一个worker线程可用之前，请求将被一直阻塞在那里。
    			如果提前知道了将要连接到服务器上的客户端数量，并且不介意运行大量线程的话，TThreadPoolServer可能是个很好的选择。

    	 4、thrift 如何实现多语言通信
    	 	1、数据传输使用socket(多种语言均支持)，数据在以特定的格式(String等)发送，接收方语言进行解析
    	 	2、定义thrift的文件，由thrift文件(IDL)生成双方语言的接口、model，在生成的model以及接口中会有解码编码的代码

     2、grpc
            grpc 是一个高性能，开源和通用的rpc框架，面向移动和HTTP/2设计。

            		调用模型

            		1、客户端（gRPC Stub）调用 A 方法，发起 RPC 调用。

            		2、对请求信息使用 Protobuf 进行对象序列化压缩（IDL）。

            		3、服务端（gRPC Server）接收到请求后，解码请求体，进行业务逻辑处理并返回。

            		4、对响应结果使用 Protobuf 进行对象序列化压缩（IDL）。

            		5、客户端接受到服务端响应，解码请求体。回调被调用的 A 方法，唤醒正在等待响应（阻塞）的客户端调用并返回响应结果。


            		Protocol Buffer 3 :是一种类似XML但更灵活和高效的结构化数据存储格式。
            		protobuf会对proto协议文件进行序列化，最终转换成二进制数据
            		优点： 快：编解码基本都是位运算，也没有复杂的嵌套关系，速度快。


            		Http2协议详解：（HTTP/2（超文本传输协议第2版，最初命名为HTTP2.0），是HTTP协议的第二个主要版本）

            		HTTP1.x的缺点：
            		1、HTTP/1.0一次只允许在一个TCP连接上发起一个请求，HTTP/1.1使用的流水线技术也只能部分处理请求并发，仍然会存在队列头阻塞问题，因此客户端在需要发起多次请求时，通常会采用建立多连接来减少延迟。
            		即（在 HTTP/1.1 协议中 「浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制。超过限制数目的请求会被阻塞」）
            		2、单向请求，只能由客户端发起。
            		3、请求报文与响应报文首部信息冗余量大。
            		4、数据未压缩，导致数据的传输量大。

            		HTTP2.0特点：

            		1、多路复用
            			多路复用允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。

            		2、二进制分帧传
            			在HTTP1.x中，我们是通过文本的方式传输数据，而HTTP2.中 在 应用层(HTTP/2)和传输层(TCP or UDP)之间增加一个二进制分帧层，采用二进制传输。其中HTTP1.x的首部信息会被封装到Headers帧，而Request Body则封装到Data帧。

            		3、服务器Push
            			在HTTP2.0中，服务端可以在客户端某个请求后，主动推送其他资源。


            源码阅读参考https://www.bookstack.cn/read/grpc-read/12-grpc%20%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC.md

     3、dubbo(没用过，java netty 等 忽略)

     4、自己动手实现geerpc(github)



数据库：

MySql （重点）

NoSQL

1、redis （重点）

2、memecache

3、monodb

4、clickhouse

