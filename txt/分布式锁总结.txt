二、 分布式锁

    分布式锁应具备以下特点：
    1、互斥性：任意时刻，同一个锁，只有一个进程能持有。
    2、安全性：避免死锁，当进程没有主动释放锁（进程崩溃退出），保证其他进程能够加锁。
    3、可用性：当提供锁的节点故障（宕机）时，"热备'即诶单能够替代故障的节点继续提供服务，并保证自身持有的数据与故障节点一致。
    4、对称性：对于同一个锁，加锁和解锁必须是同一个进程，即不能把其他进程持有的锁给释放了。

2.1 redis实现分布式锁
		基于Redis 实现分布式锁 (setnx) setnx也可以存入key,如果存入key成功则返回1，如果存入的key已经存在了，返回0.
	 正确实现1：使用多参数的set() 使用redis.set(key,value,NX,px,expire_time);
	 错误实现2: 使用redis.setnx()命令加锁，人后使用expire()方法设置过期时间。
		eg: if(redis.setnx(key)){
					reids.expire(key,expireTime);
				}

			问题：setnx（）和expire（）是两条Redis命令，不具备原子性，如果程序在执行setnx（）之后突然崩溃，会导致
			锁没有设置过期时间。将会发生死锁。网上有人这么实现，因为低版本的redis不支持多参数的set()

	错误实现3:
			 实现思路：使用jedis.setnx()命令实现加锁，其中key是锁，value是锁的过期时间。执行过程：1. 通过setnx()方法尝试加锁，如果当前锁不存在，返回加锁成功。2. 如果锁已经存在则获取锁的过期时间，和当前时间比较，如果锁已经过期，则设置新的过期时间，返回加锁成功

			 存在问题：
			 	1. 由于是客户端自己生成过期时间，所以需要强制要求分布式下每个客户端的时间必须同步。 2. 当锁过期的时候，如果多个客户端同时执行jedis.getSet()方法，那么虽然最终只有一个客户端可以加锁，但是这个客户端的锁的过期时间可能被其他客户端覆盖。3. 锁不具备拥有者标识，即任何客户端都可以解锁。
	 正确实现4:

	 		   String script = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";

        Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId));

        使用简单的lua脚本，然后将Lua代码传递到reids.eval()方法中，通过参数赋值相应的lockKey。eval()方法是将Lua代码交给Redis服务端执行，redis服务端可以确保eval()方法的原子性。


        以上策略都是单机Redis实现分布式锁，如果redis集群Master-slave会存在安全性问题：
        	例如：
        		 1、客户端1 从Master获取了锁
        		 2、Master宕机了，存储锁的key还没有来得及同步到Slave上
        		 3、Slave升级为Master
        		 4、客户端2从新的Master获取到了对应同一个资源的锁
        		 于是，客户端1和客户端2同时持有了同一个资源的锁，锁的安全型被打破


        针对redis集群多服务实例场景，可以使用Redlock算法
        可以参考Redisson实现分布式锁，Redis官方提供的分布式锁组件，内部使用了ReadLock算法。
       go的redsync包实现了redlock算法。（https://github.com/go-redsync/redsync）

        	ReadLock红锁算法：
        	1、获得当前时间（ms）

			2、首先设置一个锁有效时间valid_time，也就是超过这个时间后锁自动释放，使用相同的key和value对所有redis实例进行设置，每次链接redis实例时设置一个小于valid_time的超时时间，比如valid_time时10s，那超时时间可以设置成50ms，如果这个实例不行，那么换下一个设置

			3、计算获取锁总共占用的时间，再加上时钟偏移，如果这个总时间小于valid_time，并且成功设置锁的实例数>= N/2 + 1，那么加锁成功

			4、如果加锁成功了，那么这个锁的有效时间就是valid_time - 获取锁占用的时间 - 时钟偏移

			5、如果加锁失败，解锁所有实例（每个redis实例都运行del key）


	缺点： 自认为 只适用单实例的redis分布式锁，如果存在redis集群（主从集群，cluster集群）等，可能会存在问题（例如，master 突然挂掉，缓存key 还没有来得及同步到slave）,redis官方适用redlock算法来解决。参考java 封装的redission

	redlock算法 大致：

	公司量级不够，目前redis仅一台服务器，单实例配置，只有需要同步数据的才 进行主从配置。（一主多从）

	2.2、基于数据库悲观锁（for update） 但是存在性能问题。（用的少）

	2.3、基于etcd

        1、etcd 支持以下功能，正是依赖这些功能来实现分布式锁的：

          1、Lease 机制：即租约机制（TTL，Time To Live），Etcd 可以为存储的 KV 对设置租约，当租约到期，KV 将失效删除；同时也支持续约，即 KeepAlive。
          2、Revision 机制：每个 key 带有一个 Revision 属性值，etcd 每进行一次事务对应的全局 Revision 值都会加一，
                因此每个 key 对应的 Revision 属性值都是全局唯一的。通过比较 Revision 的大小就可以知道进行写操作的顺序。
                在实现分布式锁时，多个程序同时抢锁，根据 Revision 值大小依次获得锁，可以避免 “羊群效应”（也称 “惊群效应”），实现公平锁。
          3、Prefix 机制：即前缀机制，也称目录机制。可以根据前缀（目录）获取该目录下所有的 key 及对应的属性（包括 key, value 以及 revision 等）。
          4、Watch 机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个目录（前缀机制），当被 Watch 的 key 或目录发生变化，客户端将收到通知。

	        1、etcd 实现分布式锁 大致流程:
        	 		1、先检查/lock路径下是否有值，如果有值，说明锁已经被占用了，
        	 		2、如果没有值，写入自己的值。写入成功返回，说明加锁成功，如果写入时节点被其他节点写入过了，那么会导致加锁失败
        	 		，跳到第三步
        	 		3、监视/lock 下的时间，先入zuse
        	 		4、当/lock路径下发生事件时，当前进程被唤醒，检查发生的时间是否是删除事件（说明锁持有着主动解锁） 或者过期事件
        	 		(说明锁过期失效) 如果是的话，那么回到1，走抢锁流程。

        	 		etcd v3 api 官方已经提供了可直接使用的锁 API
        	 	 参考： https://github.com/zieckey/etcdsync/blob/master/mutex.go 实现：

        	 	 demo :使用上，跟 Golang 官方 sync 包的 Mutex 接口非常类似，先New()，然后调用Lock()，使用完后调用Unlock()，

        		 	 func main() {
        		//etcdsync.SetDebug(true)
        		log.SetFlags(log.Ldate|log.Ltime|log.Lshortfile)
        		m := etcdsync.New("/etcdsync", "123", []string{"http://127.0.0.1:2379"})
        		if m == nil {
        		log.Printf("etcdsync.NewMutex failed")
        		}
        		err := m.Lock()
        		if err != nil {
        		log.Printf("etcdsync.Lock failed")
        		} else {
        		log.Printf("etcdsync.Lock OK")
        		}

        		log.Printf("Get the lock. Do something here.")

        		err = m.Unlock()
        		if err != nil {
        		log.Printf("etcdsync.Unlock failed")
        		} else {
        			log.Printf("etcdsync.Unlock OK")
        		}
        	}




