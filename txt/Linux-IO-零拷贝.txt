Linux 基础知识

1、Linux架构层级
		Application -》 Shell 和 公共函数库 -〉系统调用 -》 内核

		Apllication: 在操作系统上安装并运行的用户态层序
		Shell : 支持编程的命令解析器
		Libs: 操作系统表标准库函数
		System Calls：暴露给用户的内核态系统调用接口
		Kernel: 操作系统的核心，真正对接硬件平台的软件程序

	操作系统将虚拟内存划分为两部分，一部分是内核空间（Kernel-space）,一部分是用户空间（User-space）。在Linux系统中
	内核模块运行在内核空间，对应的进程处于内核态，而用户程序运行在用户空间，对应的进程处于用户态。

    linux 层级：
    	用户空间 -》内核空间 =〉 硬件层。


    硬件层（hardware):
    	包括和我们熟知的和IO相关的CPU、内存、磁盘和网卡几个硬件；

    内核空间（Kernel Space）:
    	计算机开机后首先会运行内核程序，内核程序占用的一块私有的空间就是内核空间，并且可支持访问CPU所有的指令集（ring0 - ring3）以及所有的内存空间、IO及硬件设备；


    用户空间（User Space）:
    	每个普通的用户进程都有一个单独的用户空间，用户空间只能访问受限的资源（CPU的“保护模式”）也就是说用户空间是无法直接操作像内存、网卡和磁盘等硬件的；

      用户进程就可以通过系统调用访问到操作系统内核，进而就能够间接地完成对底层硬件的操作。这个访问的过程也即用户态到内核态的切换。常见的系统调用有很多，比如：内存映射mmap()、文件操作类的open()、IO读写read()、write()等等。


2、常见的I/o模型
	1、常见的I/O 分为磁盘I/O 和 网络I/O （socket等）
		I/o模型 网络编程经常能看到，不过在所有IO处理中都是类似的。
		那么对于IO的整个过程大体上分为2个部分，
		1、第一个部分为IO的调用，
		2、第二个部分为IO的执行。IO的调用指的就是系统调用，IO的执行指的是在内核中相关数据的处理过程，这个过程是由操作系统完成的，与程序员无关。
	
	2、首先我们来看看一次网络请求中服务端做了哪些操作。

		参考图片地址：https://segmentfault.com/img/bVbZMAV

		在上图中，每一个客户端会与服务端建立一次socket连接，而服务端获取连接后，对于所有的数据的读取都得经过操作系统的内核，通过系统调用内核将数据复制到用户进程的缓冲区，然后才完成客户端的进程与客户端的交互。那么根据系统调用的方式的不同分为阻塞和非阻塞，根据系统处理应用进程的方式不同分为同步和异步。


		同步/异步：关注的是消息通信机制
		　　同步：synchronous，调用者等待被调用者返回消息，才能继续执行
		　　异步：asynchronous，被调用者通过状态、通知或回调机制主动通知调用者被调用者的运行状态
		阻塞/非阻塞：关注调用者在等待结果返回之前所处的状态
		　　阻塞：blocking，指IO操作需要彻底完成后才返回到用户空间，调用结果返回之前，调用者被挂起
		　　非阻塞：nonblocking，指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成，最终的调用结果返回之前，调用者不会被挂起

	2.1 阻塞I/O (Blocking IO )

	参考图：https://segmentfault.com/img/bVbZMAY

		每一次客户端产生的socket连接实际上是一个文件描述符fd,而每一个用户进程读取的实际上也是一个个文件描述符fd,在该时期的系统调用函数会等待网络请求的数据的到达和数据从内核空间复制到用户进程空间，也就是说，无论是第一阶段的IO调用还是第二阶段的IO执行都会阻塞，那么就像图中所画的一样，对于多个客户端连接，只能开辟多个线程来处理。

	2.2 非阻塞IO模型
		对于阻塞IO模型来说最大的问题就体现在阻塞2字上，那么为了解决这个问题，系统的内核因此发生了改变。在内核中socket支持了非阻塞状态。既然这个socket是不阻塞的了，那么就可以使用一个进程处理客户端的连接，该进程内部写一个死循环，不断的询问每一个连接的网络数据是否已经到达。此时轮询发生在用户空间，但是该进程依然需要自己处理所有的连接，所以该时期为同步非阻塞IO时期，也即为NIO。
		参考图：https://segmentfault.com/img/bVbZMA6

    	阻塞不阻塞 主要是针对系统调用时 调用者是否被挂起阻塞

	2.3	I/O 复用
		在非阻塞IO模型中，虽然解决了IO调用阻塞的问题，但是产生了新的问题，如果现在有1万个连接，那么用户线程会调用1万次的系统调用read来进行处理，在用户空间这种开销太大，那么现在需要解决这个问题，思路就是让用户进程减少系统调用，但是用户自己是实现不了的，所以这就导致了内核发生了进一步变化。在内核空间中帮助用户进程遍历所有的文件描述符，将数据准备好的文件描述符返回给用户进程。该方式是同步阻塞IO，因为在第一阶段的IO调用会阻塞进程

		主要实现为：
			1、select 和poll 

				为了让内核帮助用户进程完成文件描述符的遍历，内核增加了系统调用select/poll(select与poll本质上没有什么不同，就是poll减少了文件描述符的个数限制)，现在用户进程只需要调用select系统调用函数，并且将文件描述符全部传递给select就可以让内核帮助用户进程完成所有的查询，然后将数据准备好的文件描述符再返回给用户进程，最后用户进程依次调用其他系统调用函数完成IO的执行过程
				参考流程图：
					https://segmentfault.com/img/bVbZMBe


					2.3.1 select

						Select会将全量fd_set从用户空间拷贝到内核空间，并注册回调函数， 在内核态空间来判断每个请求是否准备好数据 。select在没有查询到有文件描述符就绪的情况下，将一直阻塞（I/O多路服用中提过：select是一个阻塞函数）。如果有一个或者多个描述符就绪，那么select将就绪的文件描述符置位，然后select返回。返回后，由程序遍历查看哪个fd请求有数据。

						缺点：
						1、每次调用select，都需要把fd集合从用户态拷贝到内核态，fd越多开销则越大；
						2、每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
						3、select支持的文件描述符数量有限，默认是1024。参见/usr/include/linux/posix_types.h中的定义：

						#define __FD_SETSIZE         1024 (内核中有个参数__FD_SETSIZE定义了每个FD_SET的句柄个数)

					2.3.1 poll
					int poll ( struct pollfd * fds, unsigned int nfds, int timeout);

					pollfd结构体：
					struct pollfd {
					    int fd;               /* 文件描述符 */
					    short events;         /* 等待的事件 */
					    short revents;        /* 实际发生了的事件 */
					} ;

						poll本质和select相同，将用户传入的数据拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或主动超时，被唤醒后又要再次遍历fd。它没有最大连接数的限制，原因是它是基于链表来存储的

						缺点：
							1、poll函数和select相同都是将大量文件描述符信息从用户空间拷贝到内核空间。
							2、poll函数没有解决select轮训所有文件描述符的问题。

							poll 主要解决 select 的前两个问题：
							1、通过一个 pollfd 数组向内核传递需要关注的事件消除文件句柄上限（pollfd内部实现为链表）
							2、同时使用不同字段分别标注关注事件和发生事件，来避免重复初始化。



			2、epoll 

				在select 和 poll 实现的多路复用中依然存在一些问题。
				1、用户进程需要传递所有的文件描述符，然后内核将数据准备好的文件描述符再次传递回去，这种数据的拷贝降低了IO的速度。
				2、内核依然会执行复杂度为O(n)的主动遍历操作。

				1、第一个问题，提出了一个共享空间的概念，这个空间为用户进程和内核进程所共享，并且提供了mmap系统调用，实现用户空间和内核空间到共享空间的映射，这样用户进程就可以将1万个文件描述符写到共享空间中的红黑树上，然后内核将准备就绪的文件描述符写入共享空间的链表中，而用户进程发现链表中有数据了就直接读取然后调用read执行IO即可。

				2、第二个问题，内核引入了事件驱动机制(类似于中断)，不再主动遍历所有的文件描述符，而是通过事件驱动的方式主动通知内核该文件描述符的数据准备完毕了，然后内核就将其写入链表中即可。

				参考流程图：
					https://segmentfault.com/img/bVbZMBh


			 	epoll详细实现：

			 	2.3.2 epoll

						1、epoll的相关函数
						#include <sys/epoll.h>
						int epoll_create(int size);

						int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

						int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);


						1、epoll_create
							当某个进程调用epoll_create时 内核会创建一个eventpoll对象（也就是程序中epfd所代表的对象）

						2、epoll_ctl
							向epoll中注册事件，该函数如果调用成功返回0，否则返回-1。
							该函数主要是对内核事件表的操作，涉及插入（添加监听描述符）、删除（删除被监听的描述符）、修改（修改被监听的描述符）。

							示例：
								如果通过epoll_ctl添加sock1、sock2和sock3的监视 内核会将eventpoll添加到这三个socket的等待队列中。
								当socket收到数据后，中断程序会操作eventpoll对象，而不是直接操作进程。（当socket收到数据后，中断程序会给eventpoll的“就绪列表”添加socket引用）

						3、epoll_wait
							epoll_wait的功能就是不断查看就绪队列中有没有描述符，如果没有就一直检查、直到超时。如果有就绪描述符，就将就绪描述符通知给用户

						epoll过程：
							epoll不会让每个 socket的等待队列都添加进程A引用，而是在等待队列，添加 eventPoll对象的引用。当socket就绪时，中断程序会操作eventPoll，在eventPoll中的就绪列表(rdlist)，添加scoket引用。这样的话，进程A只需要不断循环遍历rdlist，从而获取就绪的socket。从代码来看每次执行到epoll_wait，其实都是去遍历rdlist。如果rdlist为空，那么就阻塞进程。当有socket处于就绪状态，也是发中断信号，再调用对应的中断程序。此时中断程序，会把socket加到rdlist，然后唤醒进程。进程再去遍历rdlist，获取到就绪socket。
							总之：poll轮询的是所有的socket。而epoll只轮询就绪的socket。

					epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。
					补充知识：eventpoll内核事件表的底层数据结构是红黑树，rdlist就绪描述符的底层数据结构是双向链表

						epoll的优点：
						1、没有最大并发连接的限制。
						2、效率提升，只有活跃可用的FD才会调用callback函数。
						3、内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递。

						epoll的水平触发（LT）和 边缘触发（ET）

						1、LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表．

						优点：当进行socket通信的时候，保证了数据的完整输出，进行IO操作的时候，如果还有数据，就会一直的通知你。

						缺点：由于只要还有数据，内核就会不停的从内核空间转到用户空间，所有占用了大量内核资源，试想一下当有大量数据到来的时候，每次读取一个字节，这样就会不停的进行切换。内核资源的浪费严重。效率来讲也是很低的。

						2、ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知。请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once).

						优点：每次内核只会通知一次，大大减少了内核资源的浪费，提高效率。

						缺点：不能保证数据的完整。不能及时的取出所有的数据。

						应用场景： 处理大数据。使用non-block模式的socket。



		对于epoll来说在第一阶段的epoll_wait依然是阻塞的，故也是同步阻塞式IO。

	2.4、信号驱动式IO
		在IO执行的数据准备阶段，不会阻塞用户进程。当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核需要数据，然后用户进程就继续做别的事情去了，而当内核中的数据准备好之后，内核立马发给用户进程一个信号，用户进程收到信号之后，立马调用recvfrom，去查收数据。该IO模型使用的较少。
		参考流程图：
			https://segmentfault.com/img/bVbZMBl

	2.5	异步IO(AIO)
		应用进程通过 aio_read 告知内核启动某个操作，并且在整个操作完成之后再通知应用进程，包括把数据从内核空间拷贝到用户空间。信号驱动 IO 是内核通知我们何时可以启动一个 IO 操作，而异步 IO 模型是由内核通知我们 IO 操作何时完成。是真正意义上的无阻塞的IO操作，但是目前只有windows支持AIO，linux内核暂时不支持。

		参考流程图：https://segmentfault.com/img/bVbZMBt

2、Linux零拷贝技术

	可以先介绍 linux 层级，用户空间和内核空间划分 ，I/O 读取方式等。

		补充知识：

		1、Linux I/O读写方式
			Linux 提供了
				1、轮询、
				2、I/O 中断
				3、DMA 传输
			这 3 种磁盘与主存之间的数据传输机制。其中轮询方式是基于死循环对 I/O 端口进行不断检测。I/O 中断方式是指当数据到达时，磁盘主动向 CPU 发起中断请求，由 CPU 自身负责数据的传输过程。 DMA 传输则在 I/O 中断的基础上引入了 DMA 磁盘控制器，由 DMA 磁盘控制器负责数据的传输，降低了 I/O 中断操作对 CPU 资源的大量消耗。

			简单介绍下DMA：
				DMA 的全称叫直接内存存取（Direct Memory Access），是一种允许外围设备（硬件子系统）直接访问系统主内存的机制。也就是说，基于 DMA 访问方式，系统主内存于硬盘或网卡之间的数据传输可以绕开 CPU 的全程调度。目前大多数的硬件设备，包括磁盘控制器、网卡、显卡以及声卡等都支持 DMA 技术。
		
			DMA：流程图参考：https://pic2.zhimg.com/80/v2-fe93324d8dc895963e00b16466e56a55_1440w.jpg

	    2、为什么需要零拷贝。
	    	1、传统 I/O 方式存在的问题。
	    		在 Linux 系统中，传统的访问方式是通过 write() 和 read() 两个系统调用实现的，通过 read() 函数读取文件到到缓存区中，然后通过 write() 方法把缓存中的数据输出到网络端口，伪代码如下：

	    		参考流程图：https://pic1.zhimg.com/80/v2-18e66cbb4e06d1f13e4335898c7b8e8c_1440w.jpg

			read(file_fd, tmp_buf, len);
			write(socket_fd, tmp_buf, len);

			1.1 传统读操作
				当应用程序执行 read 系统调用读取一块数据的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据；如果数据不存在，则先将数据从磁盘加载数据到内核空间的读缓存（read buffer）中，再从读缓存拷贝到用户进程的页内存中。

				基于传统的 I/O 读取方式，read 系统调用会触发 2 次上下文切换，1 次 DMA 拷贝和 1 次 CPU 拷贝，发起数据读取的流程如下：

				1、用户进程通过 read() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
				2、CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
				3、CPU将读缓冲区（read buffer）中的数据拷贝到用户空间（user space）的用户缓冲区（user buffer）。
				4、上下文从内核态（kernel space）切换回用户态（user space），read 调用执行返回。

			1.2 传统写操作

				当应用程序准备好数据，执行 write 系统调用发送网络数据时，先将数据从用户空间的页缓存拷贝到内核空间的网络缓冲区（socket buffer）中，然后再将写缓存中的数据拷贝到网卡设备完成数据发送。

				基于传统的 I/O 写入方式，write() 系统调用会触发 2 次上下文切换，1 次 CPU 拷贝和 1 次 DMA 拷贝，用户程序发送网络数据的流程如下：

				1、用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
				2、CPU 将用户缓冲区（user buffer）中的数据拷贝到内核空间（kernel space）的网络缓冲区（socket buffer）。
				3、CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
				4、上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。

		3、 零拷贝方式
				零拷贝主要主要是为了减少传统I/O 读写模式过程中的必要拷贝，和 上下文切换次数。

		在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。	

			3.1 用户态直接I/O
				用户态直接 I/O 使得应用进程或运行在用户态（user space）下的库函数直接访问硬件设备，数据直接跨过内核进行传输，内核在数据传输过程除了进行必要的虚拟存储配置工作之外，不参与任何其他工作，这种方式能够直接绕过内核，极大提高了性能。

				参考图片：https://pic1.zhimg.com/80/v2-4cb0f465ebeb7ff0f5e31e8d3f790c80_1440w.jpg
				用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。

			3.2	 mmap + write （重点）
				一种零拷贝方式是使用 mmap + write 代替原来的 read + write 方式，减少了 1 次 CPU 拷贝操作。mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址，mmap + write 的伪代码如下：

				tmp_buf = mmap(file_fd, len);
				write(socket_fd, tmp_buf, len);

				使用 mmap 的目的是将内核中读缓冲区（read buffer）的地址与用户空间的缓冲区（user buffer）进行映射，从而实现内核缓冲区与应用程序内存的共享，省去了将数据从内核读缓冲区（read buffer）拷贝到用户缓冲区（user buffer）的过程，然而内核读缓冲区（read buffer）仍需将数据到内核写缓冲区（socket buffer），大致的流程如下图所示
				https://pic2.zhimg.com/80/v2-28463616753963ac9f189ce23a485e2d_1440w.jpg

				基于 mmap + write 系统调用的零拷贝方式，整个拷贝过程会发生 4 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：

				1、用户进程通过 mmap() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
				2、将用户进程的内核空间的读缓冲区（read buffer）与用户空间的缓存区（user buffer）进行内存地址映射。
				3、CPU利用DMA控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
				4、上下文从内核态（kernel space）切换回用户态（user space），mmap 系统调用执行返回。
				5、用户进程通过 write() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
				6、CPU将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
				7、CPU利用DMA控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
				8、上下文从内核态（kernel space）切换回用户态（user space），write 系统调用执行返回。


			 mmap 主要的用处是提高 I/O 性能，特别是针对大文件。对于小文件，内存映射文件反而会导致碎片空间的浪费，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。

			 mmap 的拷贝虽然减少了 1 次拷贝，提升了效率，但也存在一些隐藏的问题。当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，服务器可能因此被终止。

		3.3 sendfile （重点）
			
			sendfile 系统调用在 Linux 内核版本 2.1 中被引入，目的是简化通过网络在两个通道之间进行的数据传输过程。sendfile 系统调用的引入，不仅减少了 CPU 拷贝的次数，还减少了上下文切换的次数，它的伪代码如下：
			sendfile(socket_fd, file_fd, len);

			通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。

			参考图：https://pic3.zhimg.com/80/v2-48132735369375701f3d8ac1d6029c2a_1440w.jpg

			基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：

			1、用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
			2、CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
			3、CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
			4、CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
			5、上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

			相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是仍然有 1 次 CPU 拷贝操作。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。	 


		3.4  splice 
			sendfile 只适用于将数据从文件拷贝到 socket 套接字上，同时需要硬件的支持，这也限定了它的使用范围。Linux 在 2.6.17 版本引入 splice 系统调用，不仅不需要硬件支持，还实现了两个文件描述符之间的数据零拷贝。splice 的伪代码如下：

			splice(fd_in, off_in, fd_out, off_out, len, flags);

			splice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。

			参考图片连接：https://pic2.zhimg.com/80/v2-37cf7a8b129183c24c7b524d3fee1a29_1440w.jpg

			基于 splice 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，0 次 CPU 拷贝以及 2 次 DMA 拷贝，用户程序读写数据的流程如下：

			1、用户进程通过 splice() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
			2、CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
			3、CPU 在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline）。
			4、CPU 利用 DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
			5、上下文从内核态（kernel space）切换回用户态（user space），splice 系统调用执行返回。
			
			splice 拷贝方式也同样存在用户程序不能对数据进行修改的问题。除此之外，它使用了 Linux 的管道缓冲机制，可以用于任意两个文件描述符中传输数据，但是它的两个文件描述符参数中有一个必须是管道设备。


		3.5 写时复制
			1、在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制的引入就是 Linux 用来保护数据的。

			2、写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中。这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。


		参考链接： https://zhuanlan.zhihu.com/p/83398714
		参考书：《高性能服务器-Unix环境高级编程》





3、Reactor模型 （没有研究过但是知道 i/O 模型 及 零拷贝）

