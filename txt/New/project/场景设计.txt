
1、秒杀活动
2、微信朋友圈
3、短域名系统
4、抢红包
5、监控报警+链路追踪+日志收集
6、TopK

1、秒杀活动
    1、秒杀前的分析和准备工作
    		1、先确定秒杀业务的场景、秒杀业务场景的特点
    			1、业务场景，比如秒杀抢购，或者秒杀抢优惠券等
    			2、特点
    			   1、定时开始，秒杀时会有大量用户在同一时间，抢购同一商品，网站流量瞬时激增
    			   2、库存有限，秒杀下单数量远远大于库存数量，只有少部分用户能够秒杀成功
    			   3、操作可靠，秒杀业务流程比较简单，一般就是下单减库存。库存就是用户争夺的“资源”，
    			   	实际被消费的“资源”不能超过计划要售出的“资源”，也就是不能够被“超卖”

    		2、流量和数据估算、平台的服务器配置
    			1、数据估算。
    				在秒杀系统设计之前，可以采用业务手段 和根据以前的业务数据、日志数据，例如：预约抢购，历史数据等分析和估算这次秒杀
    					可能要存储的数据量。
    			2、向运维人员确认 当前服务的配置信息，缓存配置信息（比如 redis 集群或者单机的部署情况），数据库的部署情况（比如主从等）
    	2、“秒杀系统”的设计
    		1、系统隔离
    			在分析秒杀的特点后，我们发现秒杀活动是有计划的，并且在短时间内会爆发大量的请求。为了不影响现有的业务系统的正常运行，我们需要把它和现有的系统做隔离。
    			即使秒杀活动出现问题也不会影响现有的系统。隔离的设计思路可以从三个维度来思考。

    			1、业务隔离
    				既然秒杀是一场活动，那它一定和常规的业务不同，我们可以把它当成一个单独的项目来看。在活动开始之前，最好设计一个“热场”。
    				“热场”的形式多种多样，例如：分享活动领优惠券，领秒杀名额等等。“热场”的形式不重要，重要的是通过它获取一些准备信息。
    				例如：有可能参与的用户数，他们的地域分布，他们感兴趣的商品。为后面的技术架构提供数据支持。

    			2、技术隔离
    				 具体各端在处理秒杀中应用到的技术方案，下面详细介绍。

    			3、数据库隔离
    				秒杀活动持续时间短，瞬时数据量大。为了不影响现有数据库的正常业务，可以建立新的库或者表来处理。
    				在秒杀结束以后，需要把这部分数据同步到主业务系统中，或者查询表中。如果数据量特别巨大，到千万级别甚至上亿，建议使用分表或者分库。

    		2、客户端
    			浏览器/客户端是用户接触“秒杀系统”的入口，那么在这一层提供缓存数据就是非常必要的。
    			1、资源静态化，并事先把资源页面放到CDN上
    				秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。

    			2、前端限流
    				 1、产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求;
                     2、JS层面，限制用户在x秒之内只能提交一次请求;
            3、代理层
            	请求通过客户端入口传到代理层。由于用户的请求量大，我们需要用负载均衡加上服务器集群，来面对如此空前的压力

            	1、限流
            		一般都是使用nginx做反向代理的，可以借助nginx进行限流，（ng层支持漏斗和令牌桶算法）

            	2、风控
            		1、接入公司的风控系统，识别是黑产账号和真实用户，以及校验请求的ip频率等限制
            		一般都是采用通杀的方式，只要通过风管分析出来这个用户是真实用户的概率没有其他用户概率大，那就认为他是机器了，丢弃他的请求。
            4、应用层（后端）
            	1、业务拆离。为了避免抢购服务影响别的业务，可以将秒杀系统单独独立成一个微服务部署，采用自己的redis 和DB
            	2、库存预热。
            		1、将库存信息放到Redis缓存中，提高查询库存的效率
            		2、防止超卖。使用redis分布式锁防止资源竞争导致的超卖现象。
            		 1、redis + lua脚本的方式，并且需要处理分布式锁超时的问题。超过一定时间自动释放锁
            	3、生成订单，扣减扣除，用户这些操作是不经过缓存直达数据库的。如果在 1 s 内，有 上万个数据连接同时到达，系统的数据库会濒临崩溃。可以使用 消息队列。
            	将秒杀请求暂存在消息队列中，然后业务服务器会响应用户“秒杀结果正在计算中”，释放了系统资源之后再处理其它用户的请求。
            		1、削去秒杀场景下的峰值写流量
            			削平短暂的流量高峰，虽说堆积会造成请求被短暂延迟处理，但是只要我们时刻监控消息队列中的堆积长度，在堆积量超过一定量时，增加队列处理机数量来提升消息的处理能力就好了，而且秒杀的用户对于短暂延迟知晓秒杀的结果也是有一定容忍度的。

            		2、通过异步处理简化秒杀请求中的业务流程
            			先处理主要的业务逻辑，采用异步处理次要的业务逻辑。 比如说，主要的流程是生成订单、扣减库存；次要的流程可能是我们在下单购买成功之后会给用户发放优惠券，会增加用户的积分。
            		3、解耦，实现秒杀系统模块之间松耦合
            				1、比如数据团队对你说，在秒杀活动之后想要统计活动的数据，借此来分析活动商品的受欢迎程度、购买者人群的特点以及用户对于秒杀互动的满意程度等等指标。而我们需要将大量的数据发送给数据团队
            				2、秒杀系统产生一条购买数据后，我们可以先把全部数据发送给消息队列，然后数据团队再订阅这个消息队列的话题，这样它们就可以接收到数据，然后再做过滤和处理了。

            		预扣库存。这种方式相对复杂一些，买家下单后，库存为其保留一定的时间（如 15 分钟），超过这段时间，库存自动释放，释放后其他买家可以购买

            5、数据库层
            	1、为了防止影响其他业务，秒杀系统需要独立数据库和表。并且预估秒杀的数据量，如果秒杀数据量超过500～100万 则考虑分表
            	2、数据库部署 采用读写分离。
            	3、数据合并。
            		在秒杀活动完毕以后，需要将其和现有的数据做合并。其实，交易已经完成，合并的目的也就是查询。
            		这个合并需要根据具体情况来分析，如果对于那些“只读”的数据，对于做了读写分离的公司，可以导入到专门负责读的数据库或者NoSQL 数据库中。

            6、压力测试
            	1、对秒杀系统整体 或者关键功能 比如 扣减库存等进行压力测试，并分析压力测试的数据，提出相关的优化方案。

2、微信朋友圈
    1、数据量预估
    		准备知识：
    			1、QPS(query per second) 是单位时间内请求的数量。
    			2、TPS （transaction per second） 代表一个事务的处理，可以包涵多次请求

    			对于单个接口 QPS和TPS没有区别。当用户一次操作包涵了多个服务请求时，这个时候TPS作为这次用户操作的性能指标就更具有代表性了。
    			例如访问一个页面请求服务器3次，一次放，产生一个“T”，产生3个“Q”
    		1、假设每天有10亿微信用户在线，1亿人发朋友圈，每天有1亿条内容和评论产生。那么数据量预估是：
    			1、1亿/（24*60*60） = 1400 TPS
    				去除每个人睡觉的7小时，则TPS大概 2000TPS
    				考虑发朋友圈的峰值 一般5倍左右： 差不多 1万TPS
    				在考虑系统一定的安全系数，一般为均值的3倍左右，TPS差不多也达到 3万TPS

    			2、每天产生1亿多条动态
    			3、接口响应时间 发朋友圈800ms以内,查看朋友圈接口响应时间，500ms以内

    			综上考虑，传统关系数据库MySqL 分库分表 也难支持这么大的用户量和数据量。
    			因此考虑采用 Feed流架构，采用写模式（Push模式） + nosql (hbase 或者自研的非关系行DB)

    	2、Feed流方案设计和介绍
    		1、目前大多数带有Feed流功能的产品都包含两种Feed流
    			1、基于算法：即动态算法推荐，比如今日头条，抖音短视频
    			2、基于关注：即社交/好友关系	比如微信朋友圈/知乎
    		2、Feed流技术实现方案1 ：读扩塞
    			1、读扩散也称为 “拉模式”
    				参考流程图：https://pic1.zhimg.com/80/v2-7198ccbc4969010fd7b1810b2026a0ec_1440w.jpg
    				如图所示：每一个内容发布者都有一个自己的发件箱（我发布的内容），每当我们发出一个新帖子，都存入自己的发件箱
    				，当我们的粉丝来阅读的时候，系统首先需要拿到【粉丝关注的所有人】，然后遍历所有发布者的发件箱，取出他们所发布的所有帖子，然后依据发布时间排序展示。

    				特点：这种设计读一次Feed流，后台会扩散为N次读操作（N为关注的人）以及一次聚合操作，因此称为读扩散。
    				每次读取feed流相当于去关注者的收件箱主动拉去帖子，因此也得名-拉模式。

    				缺点：如果阅读者关注的用户比较多 在聚合数据时候，系统的开销非常大，响应延时较大。
    			2、写扩散也称为 “推模式”
    				据统计：大多数Feed流产品的读写比大概在100:1，也就是说大部分情况都是刷Feed流看别人发的朋友圈和微博，只有很少情况是自己亲自发一条朋友圈或微博给别人看。
    				因此：读扩散那种很重的读逻辑并不适合大多数场景。
    				我们宁愿让发帖的过程复杂一些，也不愿影响用户读Feed流的体验，因此稍微改造一下前面方案就有了写扩散。写扩散也称为“推模式”，这种模式会对拉模式的一些缺点做改进。
    				1、参考流程图：https://pic1.zhimg.com/80/v2-dddd8cad69604c0a0095bd96fe66455c_1440w.jpg
    				2、如上图所示：系统中每个用户除了有发件箱，也会有自己的收件箱。当发布者发表一篇帖子的时候，除了往自己发件箱记录一下之外，还会遍历发布者的所有粉丝，往这些粉丝的收件箱也投放一份相同内容。这样阅读者来读Feed流时，直接从自己的收件箱读取即可。

    				这种设计：每次发表帖子，都会扩散为M次写操作（M等于自己的粉丝数），因此成为写扩散。每篇帖子都会主动推送到所有粉丝的收件箱，因此也得名推模式。

    				这种模式可想而知：发一篇帖子，背后会涉及到很多次的写操作。通常为了发帖人的用户体验，当发布的帖子写到自己发件箱时，就可以返回发布成功。后台另外起一个异步任务，不慌不忙地往粉丝收件箱投递帖子即可。

    				缺点：
    					写扩散的好处在于通过数据冗余（一篇帖子会被存储M份副本），提升了阅读者的用户体验。通常适当的数据冗余不是什么问题，但是到了微博明星这里，完全行不通。比如目前微博粉丝量Top2的谢娜与何炅，两个人微博粉丝过亿。
    					另外：由于写扩散是异步操作，写的太慢会导致帖子发出去半天，有些粉丝依然没能看见，这种体验也不太好

    					通常写扩散适用于好友量不大的情况，比如微信朋友圈正是写扩散模式。每一名微信用户的好友上限为5000人，也就是说你发一条朋友圈最多也就扩散到5000次写操作，如果异步任务性能好一些，完全没有问题。
    			3、Feed 读写混合模式
    				1、以微博为例：当何炅这种粉丝量超大的人发帖时，将帖子写入何炅的发件箱，另外提取出来何炅粉丝当中比较活跃的那一批（这已经可以筛掉大部分了），将何炅的帖子写入他们的收件箱。当一个粉丝量很小的路人甲发帖时，采用写扩散方式，遍历他的所有粉丝并将帖子写入粉丝收件箱。

    				对于那些活跃用户登录刷Feed流时：他直接从自己的收件箱读取帖子即可，保证了活跃用户的体验。

    				当一个非活跃的用户突然登录刷Feed流时：

    				1）一方面需要读他的收件箱；
    				2）另一面需要遍历他所关注的大V用户的发件箱提取帖子，并且做一下聚合展示。
    				在展示完后：系统还需要有个任务来判断是否有必要将该用户升级为活跃用户。

    				因为有读扩散的场景存在，因此即使是混合模式，每个阅读者所能关注的人数也要设置上限，例如新浪微博限制每个账号最多可以关注2000人。

    				如果不设上限：设想一下有一位用户把微博所有账号全部关注了，那他打开关注列表会读取到微博全站所有帖子，一旦出现读扩散，系统必然崩溃（即使是写扩散，他的收件箱也无法容纳这么多的微博）。

    				读写混合模式下，系统需要做两个判断：

    				1）哪些用户属于大V，我们可以将粉丝量作为一个判断指标；
    				2）哪些用户属于活跃粉丝，这个判断标准可以是最近一次登录时间等。
    				这两处判断标准就需要在系统发展过程中动态地识别和调整，没有固定公式。

    				通常Feed流的分页入参不会使用page_size和page_num，而是使用last_id来记录上一页最后一条内容的id。前端读取下一页的时候，必须将last_id作为入参，后台直接找到last_id对应数据，再往后偏移page_size条数据，返回给前端，这样就避免了错位问题

    	3、微信朋友圈Feed流设计
    		微信朋宇圈是双向好友设计，并且每个用户的好友数量有上线5000,明显读大于写的场景。因此采用
    		Feed 写扩撒+ NoSQL

    		1、核心表
    			1、发布表动态表
    				发布动态表记录所有用户发布的动态内容
    			2、相册表：相册每个用户独立的，记录了该用户所发布的所有内容
    			3、评论表
    			4、时间线：所谓的发朋友圈 ，就是刷时间线，就是一个用户所有的朋友的发布内容
    			5、好友关系表
    				记录用户和好友的关注关系。
    				uid 	follower_uid
    				可以把用户的好友关系维护到缓存中

    			微信朋友圈的工作流程概述
    			1、比如有两个用户小王和Mary。小王和Mary各自有各自的相册，可能在同一台服务器上，也可能在不同的服务器上。现在小王上传了一张图片到自己的朋友圈。上传图片不经过微信后台服务器，而是直接上传到最近的腾讯CDN节点，所以非常快。图片上传到该CDN后，小王的微信客户端会通知微信的朋友圈CDN：这里有一个新的发布（比如叫K2），这个发布的图片URL是什么，谁能看到这些图片，等等此类的元数据，来把这个发布写到发布的表里。

    			2、在发布的表写完之后，会把这个K2的发布索引到小王的相册表里。所以相册表其实是很小的，里面只有索引指针。相册表写好了之后，会触发一个批处理的动作。这个动作就是去跟小王的每个好友说，小王有一个新的发布，请把这个发布插入到每个好友的时间线里面去。

    			3、然后比如说现在Mary上朋友圈了，而Mary是小王的一个好友。Mary拉自己的时间线的时候，时间线会告诉到有一个新的发布K2，然后Mary的微信客户端就会去根据K2的元数据去获取图片在CDN上的URL，把图片拉到本地。

    		然后这些数据表 可以根据集群部署，或者根据时间分表 。因为之前过期的评论和动态基本没有人看的。
    		点赞和评论，拉取动态后，直接通过 ID 去获取该动态的点赞和评论，



3、短域名设计
    1、短网址：顾名思义，就是将长网址缩短到一个很短的网址，用户访问这个短网址可以重定向到原本的长网址（也就是还原的过程）。这样可以达到易于记忆、转换的目的，常用于有字数限制的微博、二维码等等场景。
    	2、短地址使用场景
    		1、因字数限制我们收到的手机短信 经常会有短网址
    		2、2、短网址二维码


    	3、短网址长度和算法
    		微博的短网址服务用的是长度为7的字符串，这个字符串可以看做是62进制的数，那么最大能表示
    		62的7次方=3521614606208个网址，大约有350亿 远远大于45亿。所以长度为7就足够了
    		因此，正确答案：长度不超过7的字符串，由大小写字母加数字共62个字母组成。a-z A-Z 0-9 这些字符组成的 62 进制

    	4、一对一还是一对多映射
    		个长网址，对应一个短网址，还是可以对应多个短网址？ 这也是个重大选择问题
    		一般而言，一个长网址，在不同的地点，不同的用户等情况下，生成的短网址应该不一样，这样，在后端数据库中，可以更好的进行数据分析。如果一个长网址与一个短网址一一对应，那么在数据库中，仅有一行数据，无法区分不同的来源，就无法做数据分析了。

    		以这个7位长度的短网址作为唯一ID，这个ID下可以挂各种信息，比如生成该网址的用户名，所在网站，HTTP头部的 User Agent等信息，收集了这些信息，才有可能在后面做大数据分析，挖掘数据的价值。短网址服务商的一大盈利来源就是这些数据。
    		正确答案：一对多
    	5、如何计算短网址
    		使用发号器生成一个十进制的整数，并将其转换成一个62进制数

    	6、	如何存储?
    		如果存储短网址和长网址的对应关系？以短网址为 primary key, 长网址为value, 可以用传统的关系数据库存起来，例如MySQL, PostgreSQL，也可以用任意一个分布式KV数据库，例如Redis, LevelDB。
    	7、301还是302重定向
    			301是永久重定向，302是临时重定向。短地址一经生成就不会变化，所以用301是符合http语义的。但是如果用了301， Google，百度等搜索引擎，搜索的时候会直接展示真实地址，那我们就无法统计到短地址被点击的次数了，也无法收集用户的Cookie, User Agent 等信息，这些信息可以用来做很多有意思的大数据分析，也是短网址服务商的主要盈利来源。

    			正确答案是302重定向。

    			抓包发现新浪微博用的就是302临时重定向


    	对于该部分的讨论，我们可以认为他是整个交互的流程，具体的流程细节如下：

    		（1）用户访问短链接：http://t.cn/RuPKzRW；

    		（2）短链接服务器http://t.cn收到请求，根据URL路径RuPKzRW获取到原始的长链接（KV缓存数据库中去查找）：https://blog.csdn.net/xlgen157387/article/details/79863301；

    		（3）服务器返回302状态码，将响应头中的Location设置为：https://blog.csdn.net/xlgen157387/article/details/79863301；

    		（4）浏览器重新向https://blog.csdn.net/xlgen157387/article/details/79863301发送请求；

    		（5）返回响应；


    	8、预防攻击
    		1、如果一些别有用心的黑客，短时间内向TinyURL服务器发送大量的请求，会迅速耗光ID，怎么办呢？首先，限制IP的单日请求总数，超过阈值则直接拒绝服务	。
    		2、可以考虑介入风控系统
    		3、方案2：使用LRU本地缓存，空间换时间
    		使用固定大小的LRU缓存，存储最近N次的映射结果，这样，如果某一个链接生成的非常频繁，则可以在LRU缓存中找到结果直接返回，这是存储空间和性能方面的折中。


4、抢红包
    1、红包算法设计
    		1、等额红包
    			根据红包的金额 和指定的数量等额分配
    		2、随机红包
    			在随机金额的红包中为了保证用户体验，通常会允许给每份红包设定一个区间范围，保证用户得到的红包不至于太小也不至于太大。
    			二倍均值算法
    				1、根据每次剩余的总金额M和剩余人数N，执行M/N再乘以2的操作得到一个边界值E，
    				然后制定一个从0到E的随机区间，在这个随机区间内将产生一个随机金额R，
    				此时总金额M将更新为M-R，剩余人数N更新为N-1。再继续重复上述执行流程，以此类推，直至最终剩余人数N-1为0，即代表随机数已经产生完毕。
        2、红包业务设计
        	1、发红包模块：主要包括接受并处理用户发红包请求的逻辑处理。

        		我们将红包个数、红包id,每个红包的随机金额存入redis缓存缓存队列中 和 DB 中

    		2、抢红包模块：主要包括用户点红包和拆红包请求的逻辑处理。
    			抢红包就是典型的高并发场景，需要避免红包超发的情况。

    			1、在"点红包"的业务逻辑中，是去缓存中判断红包个数是否大于0。
    				抢红包时，查询cache，拦截红包已经抢完、用户已经抢过、红包已经过期等无效请求。

    				当用户发起抢红包请求时，若有红包则直接完成红包占有操作，同步告知用户是否抢到红包，这个过程要求快速响应。

    			2、在"拆红包"的业务逻辑中， 也是从缓存的红包随机金额队列中去读取红包金额。

    				但由于微信红包支付属于第三方调用，若抢到红包后同步调用红包支付，系统调用链又长又慢，所以红包占有和红包发放异步拆分是必然

    				我们采用一组 Worker 去消费任务队列，并调用红包支付 API，以及数据持久化操作（后续对账）。尽管红包发放调用链又长又慢，但是注意到这些 Worker 是 无状态 的，所以可以通过增加 Worker 数量，以横向扩展提高系统的处理能力。

    				 每个大红包对应两个 redis 队列，一个是未消费红包队列，另一个是已消费红包队列。开始时，把未抢的小红包全放到未消费红包队列里

    		 		先判断用户是否抢过红包，如果没有，则从未消费红包队列中取出一个小红包，再 push 到另一个已消费队列中

    			1、限流
    				1、前端限流
    					前端限制用户在 n 秒之内只能提交一次请求，虽然这种方式只能挡住小白，不过这是 99% 的用户哟，所以也必须得做。
    				2、后端限流
    					常用的后端限流方法有 漏桶算法 和 令牌桶算法。漏桶算法 主要目的是控制请求数据注入的速率，如果此时漏桶溢出，后续的请求数据会被丢弃。
    			2、防超发
    				1、库存加锁
    					可以通过加锁的方式解决资源抢占问题，但是加锁会增加系统开销，大流量下更容易拖垮系统，不过可以尝试一下基于版本号的乐观锁。
    				2、通过高速队列串行化请求


    		3、数据操作DB模块：主要包括系统整体业务逻辑处理过程中的数据记录。
    			三张表。发红包时记录红包相关信息表、发红包时生成的对应随机金额信息表以及抢红包时用户抢到的红包金额记录表
    			1、发红包记录表 red_record
    			2、红包明细金额表	red_detail
    			3、抢红包记录表 red_rob_record

    		4、缓存中间件Redis模块：主要用于缓存红包个数及红包随机金额

    		5、红包退回模块
    			红包到期为领取 则退回用户账号（延迟消息队列处理）


5、监控报警+链路追踪+日志收集
    1、没有监控报警
    	当有问题的时候 在采用profile 和 trace 进行分析
    	 未来考虑 grpc + Prometheus（普罗米修斯） + grafana

    	 1、prometheus是一个开源指标监控解决方案，指标就是指的CPU的使用率、内存使用率等数据。
    	 	Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控
    	 2、Prometheus提供的可视化图表比较简单，因此这里引入grafana作为metrics的可视化展示界面
    2、线上没有完整的日志收集
    	日志都是根据日期生成logfile.log文件，排查问题都是找运维拿log文件

    3、线上也没有链路追踪
    	目前针对分布式应用系统结合Opentracing来构建分布式追踪系统一共有两种实现方式。一种是选定一种追踪工具，按照其官方文档在我们的服务代码中加入链路追踪的实现代码，然后配合官方UI，进行链路追踪分析。另外一种就是直接构建下一代分布式微服务架构ServiceMesh，这种架构对于我们现有的服务是无侵入性的，就是不用修改现有代码，就能够实现分布式链路追踪。不过这种方式目前一般是基于Kubernetes打造的云原生应用系统，有着很高的应用门槛，需要结合实际情况进行选择

    	分布式追踪原理:
    		1、分布式追踪的原理，与监控系统，大数据埋点采集有些类似，大体上就是数据采集，数据存储，展示分析。只不过，分布式追踪的每条数据都上下关联，形成一个有向无环的链条，这也正式分布式追踪的核心所在
    		2、要想做到这一点，就需要有一个条件，那就是每一个请求，从进入我们系统开始，就必须具备一个全局的，各个服务间通用的一个ID，我们称之为 Trace ID.通过这个trace id，追踪系统就能够跟踪到，一个请求在微服务架构中所有的行踪轨迹了。


    	未来考虑接入 	Opentracing +Jaeger  搭建一个分布式链路追踪系统来实现查看整个系统的链路、性能等指标
    	1、Opentracing
    		OpenTracing主要定义了相关的协议以及接口，这样各个语言只要按照Opentracing的接口以及协议实现数据上报，那么调用信息就能统一被收集．
    	2、Jaeger 主要是通过嵌入到代码中的client来收集数据，并传输到Collector端进行存储，然后集中通过UI进行展示。
    		数据的存储部分可以选择ElasticSearch和Cassandra。这在选型上需要注意。

    未来考虑方案：
            1、通过istio+envoy 服务注册和发现，时可以指定zipkin组件从而实现链路追踪
            2、通过grpc拦截器引入jaeger实现链路追踪


6、TopK

1、海量日志数据，提取出某日访问百度次数最多的那个IP。
      首先是这一天，并且是访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。同样可以采用映射的方法，比如模1000，把整个大文件映射为1000个小文件，再找出每个小文中出现频率最大的IP（可以采用hash_map进行频率统计，然后再找出频率最大的几个）及相应的频率。然后再在这1000个最大的IP中，找出那个频率最大的IP，即为所求。
	算法思想：分而治之+Hash
	1.IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理；
	2.可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址；
	3.对于每一个小文件，可以构建一个IP为key，出现次数为value的Hash map，同时记录当前出现次数最多的那个IP地址；
	4.可以得到1024个小文件中的出现次数最多的IP，再依据常规的堆排序得到总体上出现次数最多的IP；


2、给个超过100G的logfile, log中存着IP地址, 设计算法找到出现次数最多的IP地址？
	思路：
	1、首先我们的思路就是利用哈希进行文件的切分，我们把100G大小的logfile分为1000份，那么下来差不多没一个文件就是100M左右。
	2、然后再利用哈希函数除留余数的方法分配到对应的编号文件中，然后得出每个文件中出现次数最多的IP 3、然后堆排序取得这1000个ip中出现次数最多的

3、给两个文件，分别有100亿个整数，我们只有1G内存，如何找到两个文件件交集

	思路：
	1、我们采用哈希切分，100亿个整数我们来切分为1000个文件，这样就有2000个文件，
	2、然后我们进行哈希算法，A的100亿个数根据余数不同，对应存在A0文件-A999文件，这样就完成了A的哈希切分，然后我们对B进行哈希切分，同样的思想，分出1000个文件，根据余数的不同分配到B0-B999文件中，
	3、我们最后就进行简单的对应编号的文件之间的比对。这样就能找到两个文件的交集


4、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词
   思路：
   1、因为内存限定很小，所以我们对这个1G的大文件肯定要采用分治的思想，我们可以对这个文件进行哈希拆分，分为2000个文件，那么每个文件就是500kb内容，我们这个时候对每个词进行除留余数，把词存放到对应的文件当中去，然后我们统计每个文件中出现的词和频率，（采用trie树/hash_map
   2、然后利用小堆得出出现次数最多的100个词，对于出现次数多的，然后我们把我词和出现的频率保存在文件当中
   3、这样下来，这2000个文件当中就各有100个词，然后对着2000个文件再次进行归并排序。找出其中频数最高的100个词。

5、搜索引擎会通过日志文件把用户每次检索使用的所有检索串都记录下来，每个查询串的长度为1-255字节。
	假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门。），请你统计最热门的10个查询串，要求使用的内存不能超过1G。

	典型的Top K算法，还是在这篇文章里头有所阐述。 文中，给出的最终算法是：第一步、先对这批海量数据预处理，在O（N）的时间内用Hash表完成排序；然后，第二步、借助堆这个数据结构，找出Top K，时间复杂度为N‘logK。 即，借助堆结构，我们可以在log量级的时间内查找和调整/移动。因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比所以，我们最终的时间复杂度是：O（N） + N'*O（logK），（N为1000万，N’为300万）。ok，更多，详情，请参考原文。


6、在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

	方法一：分治法
	与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果

	方法二、位图


7、给40亿个不重复的无符号整数，没排过序。给一个无符号整数，如何快速判断一个数是否在这40亿个数中。 【腾讯】
	思路：如果内存够的话，40亿个整型使用位图存储需要500M左右的空间。
	分析：位图只适合判断，查找数据是否存在！

	字节位置=数据/32;(采用位运算即为右移5位)
	方法： 方案1：申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。

	在代码中，使用的是无符号整型数据，32个二进制位，开辟数组时，一个数组元素是一个32位的整型数据，位图的思想，则这32位二进制位就可以表示32位数，原本一个数组元素只能存一个数据，40亿个数，内存将会吃不消，查找也相当困难，位图使得一个数据用一个二进制位表示，一个无符号整型的数组元素
	就可以表示32个数据，40亿个数据，有位图的方式存，会很节省空间，同时查找效率也会得到提高


8、	题目一、有一个包含100亿个URL的大文件，假设每个URL占用16B,请找出其中所有重复的URL。

	题目二、某搜索公司一天的用户搜索词汇是海量的百亿数据量，请设计一种求出每天最热top 100 词汇的可行办法。

	 1、题目一解法：
	 由于数据量很大，16*100亿B = 1600亿Byte,约等于160G。10亿Byte约等于1G大小。这种题目都是有数据资源限制的。如果说机器内存只有16G，那我们可以通过Hash算法，把原始大文件，hash分成10个小文件。或者hash成更多更小的文件，以满足机器资源限制。因为通过hash，我们可以保证相同的URL一定可以被分配到相同的小文件上。然后从每个文件里统计计算每个URL出现就可以了，可以使用hashMap存放统计结果。

	2、题目二解法：
       其实也是基于题目一来发散这个题目。每天海量的搜索数据，会先存起来。但是我们在存的时候，也是先做hash，看新进来的词汇会分配到那个节点（或者说小文件、分区），更新统计这个节点（分区）的hashmap统计结果，也就是给这个新加的词汇出现次数+1，然后这个文件，对应的hashmap统计有一个小根堆（100个node），存放着当前这个小文件top 100的最热统计，最后我们汇总多个小文件的小根堆，得到总的top 100.

