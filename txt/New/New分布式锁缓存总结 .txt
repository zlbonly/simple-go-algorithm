分布式缓存总结

1、常用的分布式缓存。
    Redis,Memcache,groupcache.
    常用Redis, Memcache项目中也有用到， 有了解go语言实现的 groupcache。并且根据groupcache，参考网上的例子实现了geecache。主要为了熟悉go项目经验和分布式缓存流程。


2、参考Redis缓存总结

3、Memcache缓存总结。

    1、Memcache是一个高性能的分布式内存对象缓存系统。它把数据存放到内存中，从而通过内存访问提速。
        它在Memcached中维护了一张很大的hash表，该表在内存中，表的结构是key-value.

        key 一般是字符串，且唯一，value可以存放字符串，数值，对象，bool，数组，null等数据。

    2、特点
        1、所有的数据都保存在内存中，存取的数据的比硬盘快，当内存满后，通过LRU算法自动删除不使用的缓存，
        但是没有考虑到容灾问题，重启服务时，所有的数据都会丢失。
        memcache本身就是为缓存而设计的服务器，因此并没有考虑数据的永久性问题。
        2、memcache是基于客户端的分布式。
            memcache尽管是"分布式"缓存服务器，但服务器并没有分布式的功能。各个Memcached服务器之间不通信，
            各自独立存取数据，不共享任何信息。服务器并不具有分布式功能，分布式部署取决于memcache客户端。


    3、memecache 的内存算法 和 分布式 实现 及过期策略。

        1、内存算法
            Memcached 利用slab allocation 机制来分配和管理内存，它按照预先规定的大小，
            将分配的内存分割成特定长度的内存块，在把尺寸相同的内存块分成组，数据在存放时，根据键值的大小
            去匹配slab大小，找到就近的slab存放，所以存在空间浪费的现象。

           解决内存浪费的方式：
            1、可以预先计算出应用存入的数据大小，或者把同一业务类型的数据存入同一个Memcached服务器中，确保存入的数据
            大小相对均匀，这样就可以减少对内存的浪费。
        2、过期策略
            memcache 使用Lru(最近最少使用)加上到期失效的策略。
            Lru:   当空间不足的时候，memcache会优先使用已经过期的数据空间，如果还不够，那么就会把最近最少使用的对象的空间释放出来使用。（
            需要注意的是 memcache的lru是作用于slab 块的 ，而不是全局，是一个区域性的lru）
            lazy Expiration（惰性过期）的侧罗：memcache不会监控是否过期，而是在外部来获取数据的时候，才检查记录的时间戳，查看对应的key是否过期，
            这样做的目的，减轻服务器的负载。

        3、分布式
                memcache分布式主要通过客户端来实现。当数据到达客户端时，客户端实现的算法会根据 键 来决定保存的memcache服务器。
                也就是说分为两步：1、选服务器 ，2、存取数据。

                客户端主要使用的是分布式算法是，【一致性hash算法】


          总结: memcache 不常用，了解的就这么多，接下来介绍lru和一致性hash算法。（并顺口介绍geecache）



   4、LRU算法 和 hash一致性算饭 参考geecache总结。

        1、Lru缓存策略
        1.1、FIFO 先进先出
            队列：队列是一种特殊的线性表，只允许在队头进行删除元素，在队尾进行插入。

        1.2、缓存淘汰（失效）算法。FIFO,LFU 和LRU
            概述：
            GeeCache 的缓存全部存储在内存中，内存是有限的，因此不可能无限制地添加数据。假定我们设置缓存能够使用的内存大小为 N，那么在某一个时间点，添加了某一条缓存记录之后，占用内存超过了 N，这个时候就需要从缓存中移除一条或多条数据了。那移除谁呢？我们肯定希望尽可能移除“没用”的数据，那如何判定数据“有用”还是“没用”呢？

          1.2.1
            FIFO (First In  First Out) 先进先出
            先进先出，也就是淘汰缓存中最老（即最早）添加的的记录。FIFO认为，最早添加的记录，其不再被使用的可能性难过比刚添加的可能性大。这种算法的实现比较简单，创建一个队列，新增记录添加到队尾，每次内存不够时，淘汰队首。但是很多场景中，部分记录虽然最早被添加，但是也是最常被访问的，而不得不因为待的时间太长而被淘汰。这类数据会被频繁的添加进缓存中，又被淘汰出去。导致缓存命中吕降低。

          1.2.2
            LFU (Least Frequently Used) 最少使用
            最少使用。也就淘汰缓存中访问频率最低的记录。LFU认为，如果数据过去被访问多次，那么将来被访问的频率也更高。LFU的实现需要维护一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰时选择访问次数最少的即可。LFU算法的命中率是比较高的，但是缺点也比较明显，维护每个记录的访问次数，对内存的消耗是很高的；另外，如果数据的访问模式发生变化，LFU需要较长的时间去适应，也就是说LFU算法受历史数据的影响比较大。例如：某个数据历史上访问次数奇高，但是在某个时间点之后就不再被访问了，但是因为历史访问次数过高，而迟迟不能被淘汰。

          1.2.3
            LRU (Least Recently Used)
            最近最少使用。相对于仅考虑时间因素的FIFO 和 仅考虑访问频率的LFU，LRU算法可以认为是相对平衡的一种淘汰算法。LRUR认为
            如果数据最近被访问过，那么将来被访问的频率也会更高。LRU算法的实现非常简单。维护一个队列，如果某条记录被访问了，则移动到队尾，
            那么队首则是最近最少访问的数据了，淘汰该记录就行。

          1.2.4 LRU 算法实现

          数据结构：字典 key-value，例如： map[key]value 和 双向链表
          说明： 1、字典map，存储键和值的映射关系。这样可以根据某个键（key）查找对应的值value的复杂度是O(1)，在字典中插入一条记录的复杂		  度也是O(1)
                2、使用双向链表实现队列。将所有的值放到双向链表中，这样，访问到某个值时，将其移动到队尾的时间复杂度是O(1)，在队尾新增一条记录以及删除一条记录的复杂度均为O（1）。


            具体实现参照：(https://github.com/zlbonly/simple-go-algorithm/blob/master/lru/lru.go 和 https://geektutu.com/post/geecache.html）
            1.2.4.1 数据结构定义

            1.2.4.2 查找
                如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
                c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）

            1.2.4.3  删除
                c.ll.Back() 取到队首节点，从链表中删除。
                delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
                更新当前所用的内存 c.nbytes。
            1.2.4.4 新增
                如果键存在，则更新对应节点的值，并将该节点移到队尾。
                不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
                更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。


            lru算法的优缺点：
                   1、lru实现简单 在频繁访问热点数据的时候效率非常高，但是他的缺点是在于 如果偶尔的批量访问不同的数据时就会造成命中率偏低。
                   例如： 频繁的访问A，接着访问其他数据直到A数据淘汰。但是 其他数据就访问这么几次，就会导致，频繁访问的A 不得不再次加入缓存。

                  2、解决方案 lru-k
                    简单介绍lru-k
                    1、lru-k算法思想
                        Lru-k的k代表最近使用的次数，因此lru可以认为是lru-1。lru-k 的主要目的是了解决lru算法"缓存污染'问题，其核心思想是将
                        "最近使用过1次"的判断标准扩展为"最近使用过k次"

                    2、工作原理
                        lru-k 维护两个队列，一个历史队列，一个缓存队列。`
                            历史队列用于记录所有缓存数据被访问的历史，只有当数据的访问次数达到k次的时候，才将数据放入缓存中。当需要淘汰数据时，
                            lru-k会淘汰第K次访问的时间距离当前时间最大的数据。

                     简单介绍就行（并没有实现。）
            目前常用的分布式缓存有memecache 和 redis

            代码实现：

            package lru

            import "container/list"

            /**
            1、在这里我们直接使用 Go 语言标准库实现的双向链表list.List。字典的定义是map[string]*list.Element，键是字符串，值是双向链表中对应节点的指针；
            2、maxBytes 是允许使用的最大内存，nbytes 是当前已使用的内存；
            3、键值对 entry 是双向链表节点的数据类型，在链表中仍保存每个值对应的 key 的好处在于，淘汰队首节点时，需要用 key 从字典中删除对应的映射。
            4、为了通用性，我们允许值是实现了 Value 接口的任意类型，该接口只包含了一个方法 Len() int，用于返回值所占用的内存大小。
            */
            type Cache struct {
            	maxBytes int64
            	nbytes   int64
            	ll       *list.List
            	cache    map[string]*list.Element
            }

            type Entry struct {
            	key   string
            	value Value
            }

            type Value interface {
            	// 	由于go么有泛型支持，使用interface{}来代替任意的类型。
            	Len() int
            }

            func New(maxBytes int64) *Cache {
            	return &Cache{
            		maxBytes: maxBytes, // 允许使用最大内存，
            		nbytes:   0,        // 当前使用内存
            		ll:       list.New(),
            		cache:    make(map[string]*list.Element),
            	}
            }

            /**
            1、如果键存在，则更新对应节点的值，并将该节点移到队尾。
            2、不存在则是新增场景，首先队尾添加新节点 &entry{key, value}, 并字典中添加 key 和节点的映射关系。
            更新 c.nbytes，如果超过了设定的最大值 c.maxBytes，则移除最少访问的节点。
            */
            func (c *Cache) Add(key string, value Value) {
            	if ele, ok := c.cache[key]; ok {
            		c.ll.MoveToFront(ele)
            		kv := ele.Value.(*Entry)
            		c.nbytes += int64(value.Len()) - int64(kv.value.Len())
            		kv.value = value
            	} else {
            		ele := c.ll.PushFront(&Entry{key: key, value: value})
            		c.cache[key] = ele
            		c.nbytes += int64(len(key)) + int64(value.Len())
            	}

            	for c.maxBytes != 0 && c.maxBytes < c.nbytes {
            		// maxbytes 设置为0 ，代表不对内存大小设限，这里和groupcache 一致，所以不为0时，才会判断是否超过了限制
            		c.RemoveOldest()
            	}

            }

            /**
            如果键对应的链表节点存在，则将对应节点移动到队尾，并返回查找到的值。
            c.ll.MoveToFront(ele)，即将链表中的节点 ele 移动到队尾（双向链表作为队列，队首队尾是相对的，在这里约定 front 为队尾）
            */
            func (c *Cache) Get(key string) (value Value, ok bool) {

            	if ele, ok := c.cache[key]; ok {
            		c.ll.MoveToFront(ele)
            		kv := ele.Value.(*Entry)
            		return kv.value, true
            	}
            	return
            }

            /*
            	c.ll.Back() 取到队首节点，从链表中删除。
            	delete(c.cache, kv.key)，从字典中 c.cache 删除该节点的映射关系。
            	更新当前所用的内存 c.nbytes。
            */
            func (c *Cache) RemoveOldest() {
            	ele := c.ll.Back()
            	if ele != nil {
            		c.ll.Remove(ele)
            		kv := ele.Value.(*Entry)
            		delete(c.cache, kv.key)
            		c.nbytes -= int64(len(kv.key)) + int64(kv.value.Len())
            	}
            }

            func (c *Cache) Len() int {
            	return c.ll.Len()
            }

            /**
            说明： 该lru只是一个简易的实现  lru当然是比较权衡的方案，
            但是比如周期性或者突发的查询会造成缓存的命中了下降，或者说缓存污染吧，
            其实还应该介绍下lru-k的方案，lru可以认为是lru-1， 那么lru-k就可以结合lfu的一些优点了。
            */

           4、一致性哈希实现

                一致性哈希算法是啥？为什么要使用一致性哈希算法？这和分布式有什么关系？

               一致性哈希算法 是 从单节点走向分布式节点的一个重要的环节。

                举例子：

                1.1 我该访问谁？

                对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

                假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

                那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

                1.2 节点数量变化了怎么办？

                简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 hash(key) % 10 变成了 hash(key) % 9，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 缓存雪崩

                那如何解决这个问题呢？一致性哈希算法可以。


                算法原理：  参考链接：（https://geektutu.com/post/geecache-day4.html）
                一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

                1、计算节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值，放置在环上。
                2、计算 key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器。

                也就是说，一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点，这就解决了上述的问题。

                2.2 数据倾斜问题

                如果服务器的节点过少，容易引起 key 的倾斜。例如上面例子中的 peer2，peer4，peer6 分布在环的上半部分，下半部分是空的。那么映射到环下半部分的 key 都会被分配给 peer2，key 过度向 peer2 倾斜，缓存节点间负载不均。

                为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

                假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

                第一步，计算虚拟节点的 Hash 值，放置在环上。
                第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

                虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

                算法实现（go） 参考：https://github.com/zlbonly/simple-go-algorithm/blob/master/consistent/hash/consistenthash.go

        demo
        package hash

        import (
        	"fmt"
        	"hash/crc32"
        	"sort"
        	"strconv"
        )

        // 定义函数类型Hash，采取依赖注入的方式，允许用于替换成自定义的Hash函数，也方便测试替换，默认使用crc32.ChecksumIEEE 算法
        type Hash func(data []byte) uint32

        type Map struct {
        	hash     Hash
        	replicas int            // 虚拟节点倍数
        	keys     []int          // hash环keys
        	hashMap  map[int]string // 虚拟节点与真实节点映射表（键是虚拟节点的哈希值，值是真实节点的名称）
        }

        func New(replicas int, fn Hash) *Map {
        	m := &Map{
        		hash:     fn,
        		replicas: replicas,
        		hashMap:  make(map[int]string),
        	}
        	if m.hash == nil {
        		m.hash = crc32.ChecksumIEEE
        	}

        	return m
        }

        /**
        Add 函数允许传入 0 或 多个真实节点的名称。
        对每一个真实节点 key，对应创建 m.replicas 个虚拟节点，虚拟节点的名称是：strconv.Itoa(i) + key，即通过添加编号的方式区分不同虚拟节点。
        使用 m.hash() 计算虚拟节点的哈希值，使用 append(m.keys, hash) 添加到环上。
        在 hashMap 中增加虚拟节点和真实节点的映射关系。
        最后一步，环上的哈希值排序。
        */
        func (m *Map) Add(keys ...string) {
        	for _, key := range keys {
        		for i := 0; i < m.replicas; i++ {
        			hash := int(m.hash([]byte(strconv.Itoa(i) + key)))
        			m.keys = append(m.keys, hash)
        			m.hashMap[hash] = key
        		}
        	}
        	sort.Ints(m.keys)
        }

        /**
        选择节点就非常简单了，第一步，计算 key 的哈希值。
        第二步，顺时针找到第一个匹配的虚拟节点的下标 idx，从 m.keys 中获取到对应的哈希值。如果 idx == len(m.keys)，说明应选择 m.keys[0]，因为 m.keys 是一个环状结构，所以用取余数的方式来处理这种情况。
        第三步，通过 hashMap 映射得到真实的节点。
        */
        func (m *Map) Get(key string) string {
        	if len(m.keys) == 0 {
        		return ""
        	}

        	hash := int(m.hash([]byte(key)))
        	idx := sort.Search(len(m.keys), func(i int) bool {
        		return m.keys[i] >= hash
        	})

        	fmt.Print(idx)
        	return m.hashMap[m.keys[idx%len(m.keys)]]
        }